{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a51f70-1ab3-4781-adb5-c8f74d044c78",
   "metadata": {},
   "source": [
    "# 1. Duplicates analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cae2577-17d1-4774-b7b1-38e9c4b70708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../../data/tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e468b3-1bd9-4298-80a2-8058e74b82a9",
   "metadata": {},
   "source": [
    "Check duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99fc03a-b464-43e6-8c8e-2b38c91001e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()   # 36 duplicated entries on both tweet_text and cyberbullying_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb110d8-9563-4231-be0c-567db65cc75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Our pancakes are selling like hotcakes Shaz - ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>This is the opportunity to prove ourselves lik...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>Our pancakes are selling like hotcakes Shaz - ...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>@TVWEEKmag: There is only 1 way to stay in the...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>It wouldn't be fair. Kat knows NOTHING of fair...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20604</th>\n",
       "      <td>A Pakistani court has sentenced 86 members of ...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41403</th>\n",
       "      <td>Still, Davis, who is gay, said he pays a socia...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46915</th>\n",
       "      <td>Racism won't stop as long as u stil select ur ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46962</th>\n",
       "      <td>Still, Davis, who is gay, said he pays a socia...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47397</th>\n",
       "      <td>Racism won't stop as long as u stil select ur ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type\n",
       "829    Our pancakes are selling like hotcakes Shaz - ...  not_cyberbullying\n",
       "1712   This is the opportunity to prove ourselves lik...  not_cyberbullying\n",
       "1758   Our pancakes are selling like hotcakes Shaz - ...  not_cyberbullying\n",
       "1984   @TVWEEKmag: There is only 1 way to stay in the...  not_cyberbullying\n",
       "2611   It wouldn't be fair. Kat knows NOTHING of fair...  not_cyberbullying\n",
       "...                                                  ...                ...\n",
       "20604  A Pakistani court has sentenced 86 members of ...           religion\n",
       "41403  Still, Davis, who is gay, said he pays a socia...          ethnicity\n",
       "46915  Racism won't stop as long as u stil select ur ...          ethnicity\n",
       "46962  Still, Davis, who is gay, said he pays a socia...          ethnicity\n",
       "47397  Racism won't stop as long as u stil select ur ...          ethnicity\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated(keep=False)]       # Duplicated with equal tweet_text and equal cyberbullying_type     (DROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc9a14c-c987-455a-b448-d0ea8e00cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19706a3d-da50-4518-b774-6f64be9c6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r\"../../data/updated_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccb83a-5db0-4b40-b7e1-486bb7d18696",
   "metadata": {},
   "source": [
    "### Now, we also observe tweets that are identical in tweet text but differ in cyberbullying type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e35560ac-d875-4e34-ad5f-bc7d437df791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1639"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated(subset=['tweet_text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1ebe0c-e360-46f4-b7aa-115502b18a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = df[df.duplicated(subset=['tweet_text'], keep = False)]      \n",
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39669d-7369-41b6-8509-16c6c0657068",
   "metadata": {},
   "source": [
    "At this point, we have found 1639 pairs of duplicates on tweet_text. Let's observe how they are divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8afd0c6-1d53-4ee2-941a-dcb850a1ab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[d['cyberbullying_type']=='religion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e9bcea-6bde-4918-96d9-f785e16933fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[d['cyberbullying_type']=='ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdcb0fb1-3884-4b6e-91f4-7892ecf49b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[d['cyberbullying_type']=='age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42457df-cd0d-4a32-89f4-09e777508f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[d['cyberbullying_type']=='gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76aeff39-19bd-4f46-8a47-9a47599c7f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[d['cyberbullying_type']=='not_cyberbullying'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51fbbc5-b706-4596-9e3c-80c286f8d698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[d['cyberbullying_type']=='other_cyberbullying'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb6c72-19c4-4534-843b-f417cfe2e19a",
   "metadata": {},
   "source": [
    "Now, we ask ourselves: how many of these pair are composed of 'other_cyberbullying' and 'not_cyberbullying' as types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6fd18c-dc86-4893-ad13-513e410cfcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = d[(d['cyberbullying_type'] == 'other_cyberbullying') | (d['cyberbullying_type'] == 'not_cyberbullying')]\n",
    "dataset.duplicated(subset=['tweet_text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf11f71-0001-4d1e-9ba1-26f556b9751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d[d['tweet_text'] == \"@stockputout everything but mostly my priest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "760963ff-7b91-47e4-86f2-f24a62f41d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d[d['tweet_text'] == \"@Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25099909-aad2-4cda-a638-ab1806a4a5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a048b9-561e-4a4a-8fb7-68a6a06bdb7f",
   "metadata": {},
   "source": [
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78bde9a3-4d5c-4ce5-92fd-c286a95b3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.11/site-packages (2.11.0)\n",
      "Requirement already satisfied: contractions in /opt/anaconda3/lib/python3.11/site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/anaconda3/lib/python3.11/site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /opt/anaconda3/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /opt/anaconda3/lib/python3.11/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e47a928-1951-436e-a6b6-810b8d9c338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alessia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alessia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/alessia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ab9be8-d755-476f-be33-d4801db23364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import contractions\n",
    "from functools import reduce\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271741f8-7af6-4bfa-8ef6-6037d260ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening .csv file\n",
    "#df = pd.read_csv('../../data/updated_tweets.csv')\n",
    "\n",
    "#Define stop words for text cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.add(\"mkr\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9cf31c-06d4-4e42-a3f3-e96231749b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all textual emojis in the string with null character\n",
    "def remove_textual_emojis(tweet):\n",
    "    tweet = re.sub(r\"(:[DPp3Oo\\(\\)])|(:'[\\)\\(])|(;[\\)\\(])|(-.-)|(^[._]^)|(x[\\(\\)])\", '', tweet)\n",
    "    return tweet #emoji.replace_emoji(tweet,'')\n",
    "\n",
    "\n",
    "#Replace links and mentions (@) with null character\n",
    "def remove_links_mentions(tweet):\n",
    "    return re.sub(r\"((?:\\@|https?\\:\\/\\/|www)\\S+)|(^RT)\", '', tweet)\n",
    "\n",
    "\n",
    "#Remove all hashtags at the end of the sentence and remove the # symbol from all others\n",
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub(r\"(\\s+#[\\w-]+)+\\s*$\", '', tweet)\n",
    "    return re.sub(r\"#([\\w-]+)\", r'\\1', tweet)\n",
    "\n",
    "\n",
    "#Remove multiple spaces (2+) and remove spaces at the beginning and end of the sentence \n",
    "def remove_spaces(tweet):\n",
    "    tweet = re.sub(r\"\\s{2,}\", ' ', tweet)\n",
    "    tweet = re.sub(r\"^\\s\", '', tweet)\n",
    "    return re.sub(r\"\\s$\", '', tweet)\n",
    "\n",
    "\n",
    "#Remove not ASCII characters (it includes not-textual emojis)\n",
    "def remove_not_ASCII(tweet):\n",
    "    return tweet.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "\n",
    "#Expand contractions (e.g.: can't => cannot)\n",
    "def remove_contractions(tweet):\n",
    "    return contractions.fix(tweet,slang=True)\n",
    "\n",
    "\n",
    "#Remove all stopwords\n",
    "def remove_stopwords(tweet):\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # checks whether they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "#Remove punctuation symbols\n",
    "def remove_punctuation(tweet):\n",
    "    return remove_spaces(tweet.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "#Controllare\n",
    "def lemmatization(tweet):\n",
    "    return lemmatizer.lemmatize(tweet)\n",
    "\n",
    "\n",
    "#Stemming\n",
    "#Ok\n",
    "def stemming(tweet):\n",
    "    tmp_tweet = word_tokenize(tweet)\n",
    "    return reduce(lambda x, y: x + ps.stem(y), tmp_tweet, \"\")\n",
    "\n",
    "\n",
    "#Remove multiple doubles in each word of the tweet (e.g.: coooll => cooll)\n",
    "def remove_elongated_words(tweet):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "\n",
    "#Remove & and $ symbols\n",
    "def remove_special_characters(tweet):\n",
    "    return re.sub(r'[&$]', '',tweet)\n",
    "\n",
    "\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503f31a9-dfd4-454b-9c17-9d478cfff076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    tweet = remove_contractions(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = remove_elongated_words(tweet)\n",
    "    \n",
    "    tweet = lemmatization(tweet)\n",
    "    #tweet = stemming(tweet)\n",
    "\n",
    "    tweet = remove_short_tweets(tweet, min_words=3)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fe5c6c7-6b78-47df-9796-dccf221c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply normalization to all tweets\n",
    "df.tweet_text = [normalize_tweet(tweet) for tweet in df.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41f9970e-61c6-4994-a8d5-bf10e9cff68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete empty tweets after the normalization\n",
    "df=df.drop(df[(df.tweet_text == r'')].index)  # Rimosse 273 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "570ef948-7837-4a98-8e60-f292ed832068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Export normalizated tweets\n",
    "df.to_csv(r\"../../data/normalized_tweets.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
