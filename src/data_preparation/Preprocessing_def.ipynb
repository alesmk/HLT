{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a048b9-561e-4a4a-8fb7-68a6a06bdb7f",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "e3ab9be8-d755-476f-be33-d4801db23364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import contractions\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Da inserire nelle dipendenze\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "295994b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "271741f8-7af6-4bfa-8ef6-6037d260ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stop words for text cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.add(\"mkr\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "eb9cf31c-06d4-4e42-a3f3-e96231749b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all textual emojis in the string with null character\n",
    "def remove_textual_emojis(tweet): \n",
    "    tweet = re.sub(r\"(:[DPp3Oo\\(\\)])|(:'[\\)\\(])|(;[\\)\\(])|(-.-)|(^[._]^)|(x[\\(\\)])\", '', tweet)\n",
    "    return tweet \n",
    "\n",
    "\n",
    "#Replace links and mentions (@) with null character\n",
    "def remove_links_mentions(tweet): \n",
    "    return re.sub(r\"((?:\\@|https?\\:\\/\\/|www)\\S+)|(^RT)\", '', tweet)\n",
    "\n",
    "\n",
    "#Remove all hashtags at the end of the sentence and remove the # symbol from all others\n",
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub(r\"(\\s+#[\\w-]+)+\\s*$\", '', tweet)\n",
    "    return re.sub(r\"#([\\w-]+)\", r'\\1', tweet)\n",
    "\n",
    "\n",
    "#Remove multiple spaces (2+) and remove spaces at the beginning and end of the sentence \n",
    "def remove_spaces(tweet):\n",
    "    tweet = re.sub(r\"\\s{2,}\", ' ', tweet)\n",
    "    tweet = re.sub(r\"^\\s\", '', tweet)\n",
    "    return re.sub(r\"\\s$\", '', tweet)\n",
    "\n",
    "\n",
    "#Remove not ASCII characters (it includes not-textual emojis)\n",
    "def remove_not_ASCII(tweet):\n",
    "    return tweet.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "\n",
    "#Expand contractions (e.g.: can't => cannot)\n",
    "def remove_contractions(tweet):\n",
    "    return contractions.fix(tweet,slang=True)\n",
    "\n",
    "\n",
    "#Remove all stopwords\n",
    "def remove_stopwords(tweet):\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # checks whether they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "#Remove punctuation symbols\n",
    "def remove_punctuation(tweet):\n",
    "    return remove_spaces(tweet.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "\n",
    "#Stemming\n",
    "def stemming(tweet):\n",
    "    tmp_tweet = word_tokenize(tweet)\n",
    "    return reduce(lambda x, y: x + ps.stem(y), tmp_tweet, \"\")\n",
    "\n",
    "\n",
    "#Remove multiple doubles in each word of the tweet (e.g.: coooll => cooll)\n",
    "def remove_elongated_words(tweet):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "\n",
    "#Remove & and $ symbols\n",
    "def remove_special_characters(tweet):\n",
    "    return re.sub(r'[&$]|amp', '',tweet)\n",
    "\n",
    "\n",
    "#Remove short tweets\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "f2e63525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check words spelling\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(tweet):\n",
    "    words = word_tokenize(tweet)\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            correction = spell.correction(word)\n",
    "            corrected_words.append(correction if correction is not None else word)\n",
    "        corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Combine wordnet and the standard list of words in NLTK\n",
    "english_words = set(words.words())\n",
    "english_words.update(w for w in wordnet.words())\n",
    "\n",
    "def remove_non_english(tweet):\n",
    "    words = word_tokenize(tweet)\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in english_words:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "9204a2ad-a00e-4816-ab42-73c43373f047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatization_with_pos(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tagged_words = pos_tag(word_tokenize(sentence))\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        wntag = tag[0].lower()  # Get the first character of the tag\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None  # Map POS tags to WordNet POS tags\n",
    "        if not wntag:\n",
    "            lemma = word  # If the POS tag is not recognized, use the word as is\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, pos=wntag)  # Lemmatize the word with its POS tag\n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "47249d30-dd73-42a3-ac49-95f18662415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_remove(df):\n",
    "    empty_tweet_indices = df[df['tweet_text'] == ''].index.tolist()\n",
    "    duplicate_indices = df[df.duplicated()].index.tolist()\n",
    "    \n",
    "    indices_to_remove = list(set(empty_tweet_indices + duplicate_indices))\n",
    "    \n",
    "    return indices_to_remove\n",
    "\n",
    "def remove_indices_from_all_datasets(df, indices):\n",
    "    for dataset in df:\n",
    "        dataset = dataset.drop(indices, inplace=True)\n",
    "        dataset = dataset.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "def clean_normalized_df(df):\n",
    "    #Delete empty tweets after the normalization\n",
    "    df=df.drop(df[(df.tweet_text == r'')].index)  # Rimosse 273 entries\n",
    "    \n",
    "    #Check duplicates after the normalization\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89873add",
   "metadata": {},
   "source": [
    "For each model, we need apply different normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "503f31a9-dfd4-454b-9c17-9d478cfff076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_normalization(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    tweet = remove_contractions(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = remove_elongated_words(tweet)\n",
    "    \n",
    "    tweet = lemmatization_with_pos(tweet)\n",
    "    \n",
    "    tweet = remove_short_tweets(tweet, min_words=3)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def Bayes_preprocessing(df, ts = 'no'):\n",
    "    df['tweet_text'] = df['tweet_text'].apply(Bayes_normalization)\n",
    "    i = []\n",
    "    if ts == 'no':\n",
    "        df = clean_normalized_df(df)\n",
    "    else:\n",
    "        i = index_to_remove(df)\n",
    "    return df, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "1fe5c6c7-6b78-47df-9796-dccf221c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformers_normalization(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    return tweet\n",
    "\n",
    "def Transformers_preprocessing(df, ts = 'no'):\n",
    "    df['tweet_text'] = df['tweet_text'].apply(Transformers_normalization)\n",
    "    i = []\n",
    "    if ts == 'no':\n",
    "        df = clean_normalized_df(df)\n",
    "    else:\n",
    "        i = index_to_remove(df)\n",
    "    return df, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "f5b2503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_normalization(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    tweet = remove_contractions(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = remove_elongated_words(tweet)\n",
    "    \n",
    "    tweet = correct_spelling(tweet)\n",
    "    tweet = remove_non_english(tweet)\n",
    "    tweet = remove_short_tweets(tweet, min_words=3)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def LSTM_normalization2(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    tweet = remove_contractions(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = remove_elongated_words(tweet)\n",
    "\n",
    "    tweet = remove_short_tweets(tweet, min_words=3)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def LSTM_preprocessing(df, preprocessing = 'yes', ts = 'no'):\n",
    "    i = [] \n",
    "    if preprocessing == 'yes':\n",
    "        df['tweet_text'] = df['tweet_text'].apply(LSTM_normalization)\n",
    "        if ts == 'no':\n",
    "            df = clean_normalized_df(df)\n",
    "        else:\n",
    "            i = index_to_remove(df)\n",
    "    else:\n",
    "        df['tweet_text'] = df['tweet_text'].apply(LSTM_normalization2)\n",
    "        if ts == 'no':\n",
    "            df = clean_normalized_df(df)\n",
    "        else:\n",
    "            i = index_to_remove(df)\n",
    "    return df, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6858d",
   "metadata": {},
   "source": [
    "Steps to ensure that all models have the exact same test set (because after different kinds of normalization it can change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "eb244d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/chiarapiccolo/Documents/GitHub/HLT/data/test_tweets.csv')\n",
    "ts_bayes, bayes_indices = Bayes_preprocessing(df, 'yes')\n",
    "\n",
    "df = pd.read_csv('/Users/chiarapiccolo/Documents/GitHub/HLT/data/test_tweets.csv')\n",
    "ts_transformers, trans_indices = Transformers_preprocessing(df, 'yes')\n",
    "\n",
    "df = pd.read_csv('/Users/chiarapiccolo/Documents/GitHub/HLT/data/test_tweets.csv')\n",
    "ts_LSTM_pro, lstm_indices_pro = LSTM_preprocessing(df, 'yes', 'yes')\n",
    "\n",
    "df = pd.read_csv('/Users/chiarapiccolo/Documents/GitHub/HLT/data/test_tweets.csv')\n",
    "ts_LSTM_no, lstm_indices_no = LSTM_preprocessing(df, 'no', 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "3c077d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti gli indici in set se non lo sono già\n",
    "bayes_indices = set(bayes_indices)\n",
    "trans_indices = set(trans_indices)\n",
    "lstm_indices_pro = set(lstm_indices_pro)\n",
    "lstm_indices_no = set(lstm_indices_no)\n",
    "\n",
    "all_indices = trans_indices.union(bayes_indices).union(lstm_indices_no).union(lstm_indices_pro)\n",
    "\n",
    "# Rimuovi gli indici dal DataFrame originale\n",
    "df_bayes = ts_bayes.drop(all_indices, axis=0)\n",
    "df_transformers = ts_transformers.drop(all_indices, axis=0)\n",
    "df_LSTM_pro = ts_LSTM_pro.drop(all_indices, axis=0)\n",
    "df_LSTM_no = ts_LSTM_no.drop(all_indices, axis=0)\n",
    "\n",
    "df_bayes.to_csv(\"TS_NB.csv\", index=False)\n",
    "df_transformers.to_csv(\"TS_T.csv\", index=False)\n",
    "df_LSTM_pro.to_csv(\"TS_LSTM_P.csv\", index=False)\n",
    "df_LSTM_no.to_csv(\"TS_LSTM_NP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "900dee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set distirbution for LSTM_pro:\n",
      "cyberbullying_type\n",
      "religion             1614\n",
      "age                  1568\n",
      "ethnicity            1502\n",
      "gender               1497\n",
      "not_cyberbullying    1338\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distirbution for LSTM_nopro:\n",
      "cyberbullying_type\n",
      "religion             1614\n",
      "age                  1568\n",
      "ethnicity            1502\n",
      "gender               1497\n",
      "not_cyberbullying    1338\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_countsts = df_LSTM_pro['cyberbullying_type'].value_counts()\n",
    "print(\"Test set distirbution for LSTM_pro:\")\n",
    "# Stampa i conteggi per ogni classe\n",
    "print(class_countsts)\n",
    "print()\n",
    "\n",
    "# Supponendo che il tuo DataFrame si chiami df_train\n",
    "class_countsts = df_LSTM_no['cyberbullying_type'].value_counts()\n",
    "print(\"Test set distirbution for LSTM_nopro:\")\n",
    "# Stampa i conteggi per ogni classe\n",
    "print(class_countsts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "abccfde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set distirbution for Naive Bayes:\n",
      "cyberbullying_type\n",
      "religion             1614\n",
      "age                  1568\n",
      "ethnicity            1502\n",
      "gender               1497\n",
      "not_cyberbullying    1338\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set distirbution for BERT:\n",
      "cyberbullying_type\n",
      "religion             1614\n",
      "age                  1568\n",
      "ethnicity            1502\n",
      "gender               1497\n",
      "not_cyberbullying    1338\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_countsts = df_bayes['cyberbullying_type'].value_counts()\n",
    "print(\"Test set distirbution for Naive Bayes:\")\n",
    "# Stampa i conteggi per ogni classe\n",
    "print(class_countsts)\n",
    "print()\n",
    "\n",
    "# Supponendo che il tuo DataFrame si chiami df_train\n",
    "class_countsts = df_transformers['cyberbullying_type'].value_counts()\n",
    "print(\"Test set distirbution for BERT:\")\n",
    "# Stampa i conteggi per ogni classe\n",
    "print(class_countsts)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a6d21",
   "metadata": {},
   "source": [
    "Creation of test and validation sets for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "a180644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = {\"../../data/train_tweets.csv\", \"../../data/eval_tweets.csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "6129a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df, _ = Bayes_preprocessing(df, 'no')\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_Naive_Bayes_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "79b39403",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df, _= LSTM_preprocessing(df, 'yes', 'no')\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_LSTM_pre_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "93a11a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df, _ = LSTM_preprocessing(df, 'no', 'no')\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_LSTM_no_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "98a7706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df, _= Transformers_preprocessing(df, 'no')\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_Transformers_new.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cc9b73884fc6dc94cb042d029db4d21e2f9331e7b6a549688c5b94abae7d991"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('ispr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
