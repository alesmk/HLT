{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a048b9-561e-4a4a-8fb7-68a6a06bdb7f",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3ab9be8-d755-476f-be33-d4801db23364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import contractions\n",
    "from functools import reduce\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "271741f8-7af6-4bfa-8ef6-6037d260ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stop words for text cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.add(\"mkr\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb9cf31c-06d4-4e42-a3f3-e96231749b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all textual emojis in the string with null character\n",
    "def remove_textual_emojis(tweet): \n",
    "    tweet = re.sub(r\"(:[DPp3Oo\\(\\)])|(:'[\\)\\(])|(;[\\)\\(])|(-.-)|(^[._]^)|(x[\\(\\)])\", '', tweet)\n",
    "    return tweet #emoji.replace_emoji(tweet,'')\n",
    "\n",
    "\n",
    "#Replace links and mentions (@) with null character\n",
    "def remove_links_mentions(tweet): \n",
    "    return re.sub(r\"((?:\\@|https?\\:\\/\\/|www)\\S+)|(^RT)\", '', tweet)\n",
    "\n",
    "\n",
    "#Remove all hashtags at the end of the sentence and remove the # symbol from all others\n",
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub(r\"(\\s+#[\\w-]+)+\\s*$\", '', tweet)\n",
    "    return re.sub(r\"#([\\w-]+)\", r'\\1', tweet)\n",
    "\n",
    "\n",
    "#Remove multiple spaces (2+) and remove spaces at the beginning and end of the sentence \n",
    "def remove_spaces(tweet):\n",
    "    tweet = re.sub(r\"\\s{2,}\", ' ', tweet)\n",
    "    tweet = re.sub(r\"^\\s\", '', tweet)\n",
    "    return re.sub(r\"\\s$\", '', tweet)\n",
    "\n",
    "\n",
    "#Remove not ASCII characters (it includes not-textual emojis)\n",
    "def remove_not_ASCII(tweet):\n",
    "    return tweet.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "\n",
    "#Expand contractions (e.g.: can't => cannot)\n",
    "def remove_contractions(tweet):\n",
    "    return contractions.fix(tweet,slang=True)\n",
    "\n",
    "\n",
    "#Remove all stopwords\n",
    "def remove_stopwords(tweet):\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # checks whether they are present in stop_words or not\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "#Remove punctuation symbols\n",
    "def remove_punctuation(tweet):\n",
    "    return remove_spaces(tweet.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "\n",
    "#Lemmatization\n",
    "#Controllare\n",
    "def lemmatization(tweet):\n",
    "    return lemmatizer.lemmatize(tweet)\n",
    "\n",
    "\n",
    "#Stemming\n",
    "#Ok\n",
    "def stemming(tweet):\n",
    "    tmp_tweet = word_tokenize(tweet)\n",
    "    return reduce(lambda x, y: x + ps.stem(y), tmp_tweet, \"\")\n",
    "\n",
    "\n",
    "#Remove multiple doubles in each word of the tweet (e.g.: coooll => cooll)\n",
    "def remove_elongated_words(tweet):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "\n",
    "#Remove & and $ symbols\n",
    "def remove_special_characters(tweet):\n",
    "    return re.sub(r'[&$]|amp', '',tweet)\n",
    "\n",
    "\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9204a2ad-a00e-4816-ab42-73c43373f047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatization_with_pos(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tagged_words = pos_tag(word_tokenize(sentence))\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        wntag = tag[0].lower()  # Get the first character of the tag\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None  # Map POS tags to WordNet POS tags\n",
    "        if not wntag:\n",
    "            lemma = word  # If the POS tag is not recognized, use the word as is\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, pos=wntag)  # Lemmatize the word with its POS tag\n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47249d30-dd73-42a3-ac49-95f18662415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_normalized_df(df):\n",
    "    #Delete empty tweets after the normalization\n",
    "    df=df.drop(df[(df.tweet_text == r'')].index)  # Rimosse 273 entries\n",
    "    \n",
    "    #Check duplicates after the normalization\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "503f31a9-dfd4-454b-9c17-9d478cfff076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_normalization(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    tweet = remove_contractions(tweet)\n",
    "    tweet = remove_stopwords(tweet)\n",
    "    tweet = remove_punctuation(tweet)\n",
    "    tweet = remove_elongated_words(tweet)\n",
    "    \n",
    "    tweet = lemmatization_with_pos(tweet)\n",
    "    #tweet = stemming(tweet)\n",
    "\n",
    "    tweet = remove_short_tweets(tweet, min_words=3)\n",
    "    return tweet\n",
    "\n",
    "def Bayes_preprocessing(df):\n",
    "    df['tweet_text'] = df['tweet_text'].apply(Bayes_normalization)\n",
    "    df = clean_normalized_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1fe5c6c7-6b78-47df-9796-dccf221c953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformers_normalization(tweet):\n",
    "    tweet = remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = remove_hashtag(tweet)\n",
    "    tweet = remove_special_characters(tweet)\n",
    " \n",
    "    tweet = remove_spaces(tweet)\n",
    "    tweet = remove_textual_emojis(tweet)\n",
    "    tweet = remove_not_ASCII(tweet)\n",
    "    return tweet\n",
    "\n",
    "def Transformers_preprocessing(df):\n",
    "    df['tweet_text'] = df['tweet_text'].apply(Transformers_normalization)\n",
    "    df = clean_normalized_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f5b2503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_normalization(tweet):\n",
    "    #Ciao Chiara, aggiungi quello che ti serve\n",
    "    return tweet\n",
    "\n",
    "def LSTM_preprocessing(df):\n",
    "    df['tweet_text'] = df['tweet_text'].apply(LSTM_normalization)\n",
    "    df = clean_normalized_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0fa32efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = {\"../../data/train_tweets.csv\", \"../../data/eval_tweets.csv\", \"../../data/test_tweets.csv\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9303e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df = Bayes_preprocessing(df)\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_Naive_Bayes.csv\", index=False)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7caf0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df = LSTM_preprocessing(df)\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_LSTM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22bc2027",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pool:\n",
    "    df = pd.read_csv(file)\n",
    "    df = Transformers_preprocessing(df)\n",
    "    file = file.split(\".csv\")[0]\n",
    "    df.to_csv(file+\"_Transformers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6044e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
