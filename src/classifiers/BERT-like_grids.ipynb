{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64db8417",
   "metadata": {},
   "source": [
    "# Grid Search for BERT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29cabaf-38ba-4c3e-8c90-9a9d713f53e9",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053931c2-bf49-434c-8ec4-9345336b2f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Alessia\\UniProjects\\HLT\\src\\classifiers\\..\\data_preparation\\Preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import import_ipynb\n",
    "from data_preparation import Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82b058-b26c-449a-bcac-4fcfefd2f5a3",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87b0ca-806c-463d-a27f-9a7dc35e19ea",
   "metadata": {},
   "source": [
    "### Tweet text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284b329c-5489-456e-af0c-c6ca0e214091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/updated_tweets.csv')\n",
    "\n",
    "def normalize_tweet_BERT(tweet):\n",
    "    tweet = Preprocessing.remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = Preprocessing.remove_hashtag(tweet)\n",
    "    tweet = Preprocessing.remove_special_characters(tweet)\n",
    " \n",
    "    tweet = Preprocessing.remove_spaces(tweet)\n",
    "    tweet = Preprocessing.remove_textual_emojis(tweet)\n",
    "    tweet = Preprocessing.remove_not_ASCII(tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "df['tweet_text'] = df['tweet_text'].apply(normalize_tweet_BERT)\n",
    "df = Preprocessing.clean_normalized_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df427bb4-5409-4666-897f-34266abf4cfa",
   "metadata": {},
   "source": [
    "### Labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d46e1a8-fdb1-48a6-ac14-02a677c38c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not_cyberbullying': 0, 'gender': 1, 'religion': 2, 'age': 3, 'ethnicity': 4}\n"
     ]
    }
   ],
   "source": [
    "possible_labels = df.cyberbullying_type.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "print(label_dict)\n",
    "df['label'] = df.cyberbullying_type.replace(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b5b24-549a-4bce-99c1-57ff9f23ced3",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8285e441-9075-4aa2-ae94-1254a267d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, checkpoint):\n",
    "    '''\n",
    "    if checkpoint == 'roberta-base':\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(checkpoint, truncation=True, do_lower_case=True)\n",
    "    else:\n",
    "        tokenizer = BertTokenizer.from_pretrained(checkpoint, do_lower_case=True)\n",
    "    '''\n",
    "    if \"roberta\" in checkpoint:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(checkpoint)\n",
    "    elif \"bert\" in checkpoint:\n",
    "        tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "    elif \"electra\" in checkpoint:\n",
    "        tokenizer = ElectraTokenizer.from_pretrained(checkpoint)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "        \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for tweet in df.tweet_text:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            tweet,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = 64,\n",
    "                            padding = 'max_length', \n",
    "                            return_attention_mask = True,\n",
    "                            truncation = True,\n",
    "                            return_tensors = 'pt',\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df.label.values)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05a782-b6e3-4439-b3f9-64e8d83e1c54",
   "metadata": {},
   "source": [
    "# Grid search preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175ca69a-f74c-40df-900b-e2c7b5a03098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, epochs, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_masks, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "            #outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "            #outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        val_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    return val_accuracy / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd4a642-1849-47a4-a308-71e4ea34cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [1e-5, 2e-5, 3e-5],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_train_epochs': [2, 3, 4]\n",
    "}\n",
    "\n",
    "def grid_search(param_grid, model, train_dataloader, val_dataloader, device):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    total_combinations = len(param_grid['learning_rate']) * len(param_grid['batch_size']) * len(param_grid['num_train_epochs'])\n",
    "    with tqdm(total=total_combinations, desc=\"Grid Search Progress\") as pbar:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for bs in param_grid['batch_size']:\n",
    "                for epochs in param_grid['num_train_epochs']:\n",
    "                    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "                    score = train_model(model, train_dataloader, val_dataloader, epochs, optimizer, device)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {'learning_rate': lr, 'batch_size': bs, 'num_train_epochs': epochs}\n",
    "                    pbar.update(1)\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b888ae-b779-4e3c-9b9c-ec03f93335d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_grid(df, checkpoint):\n",
    "    input_ids, attention_masks, labels = preprocess_data(df, checkpoint)\n",
    "\n",
    "    batch_size = 32\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "     # Model initialization\n",
    "    if \"roberta\" in checkpoint:\n",
    "        model = RobertaForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "    elif \"bert\" in checkpoint:\n",
    "        model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "    elif \"electra\" in checkpoint:\n",
    "        model = ElectraForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    '''if checkpoint == 'roberta-base':\n",
    "        model = RobertaModel.from_pretrained(checkpoint)\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=5)\n",
    "    '''\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"\\nGrid Search {checkpoint}...\")\n",
    "    best_params, best_score = grid_search(param_grid, model, train_dataloader, val_dataloader, device)\n",
    "    \n",
    "    print(f'I migliori parametri trovati sono: {best_params} con un punteggio di: {best_score}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19197edc-2d6c-435a-bec1-f9908b16dc7f",
   "metadata": {},
   "source": [
    "# Run the grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917147c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search bert-base-uncased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5f3caf2ed741e6bf8d7c28479f5218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grid Search Progress:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "execute_grid(df, \"bert-base-uncased\")\n",
    "execute_grid(df, \"bert-large-uncased-whole-word-masking\")\n",
    "execute_grid(df, \"google/electra-base-discriminator\")\n",
    "execute_grid(df, \"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6806f-7f8c-410c-9d6c-b18d559e7153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
