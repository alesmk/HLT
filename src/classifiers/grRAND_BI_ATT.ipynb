{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'F1Score' from 'sklearn.metrics' (/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m  RandomizedSearchCV\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m make_scorer, f1_score, F1Score\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'F1Score' from 'sklearn.metrics' (/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.stats import uniform\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense, Attention, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "from collections import Counter\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# define attention layer\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='attention_weights', \n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Compute attention scores\n",
    "        attention_scores = K.dot(x, self.W)\n",
    "        attention_scores = K.squeeze(attention_scores, axis=-1)\n",
    "        attention_weights = K.softmax(attention_scores)\n",
    "\n",
    "        # Apply attention weights\n",
    "        weighted_input = x * K.expand_dims(attention_weights)\n",
    "\n",
    "        # Sum over timesteps\n",
    "        context_vector = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "# Define early stopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "#f1_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# define the model\n",
    "def create_lstm_model(units, vocab_length, embedding_matrix, max_len, dropout):\n",
    "    lstm_model = Sequential()\n",
    "    embedding_layer = Embedding(vocab_length, 200, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "    lstm_model.add(embedding_layer)\n",
    "    lstm_model.add(Dropout(dropout))\n",
    "    lstm_model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "    lstm_model.add(AttentionLayer())\n",
    "    lstm_model.add(Dense(5, activation='softmax', kernel_initializer='ones'))\n",
    "    lstm_model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\")\n",
    "    return lstm_model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "#tf.random.set_seed(7)\n",
    "\n",
    "df = pd.read_csv('../../data/normalized_tweets.csv')\n",
    "df = df[df['cyberbullying_type'] != 'other_cyberbullying']\n",
    "# Reset index after filtering out the class\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "#df[\"cyberbullying_type\"].value_counts()\n",
    "\n",
    "### try different length based on tweet lentgh\n",
    "df['text_len'] = [len(text.split()) for text in df.tweet_text]\n",
    "#max_len = np.max(df['text_len'])\n",
    "#avg_len = np.mean(df['text_len'])\n",
    "#avg_len = int(avg_len)\n",
    "#print(avg_len)\n",
    "avg_len = 35\n",
    "# checks on tweets length\n",
    "count = (df['text_len'] >= 35).sum()\n",
    "print(\"Number of values greater than or equal to 35:\", count)\n",
    "\n",
    "\n",
    "X, y = df[\"tweet_text\"], df[\"cyberbullying_type\"]\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform label encoder on the target variable\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#Each word in input used as a key, while a unique index is used as the value of the key \n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(x_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_length)\n",
    "\n",
    "####\n",
    "\n",
    "X_train = pad_sequences(X_train, padding = 'pre', maxlen = avg_len)\n",
    "X_test = pad_sequences(X_test, padding = 'pre', maxlen = avg_len)\n",
    "\n",
    "# Load GloVe word embeddings and create a dictionary that willl contain words as keys, and their corresponging embedding list as values. \n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('../../glove_embeddings/glove.twitter.27B.200d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 200)) ## change if the dimention of embedding changes above\n",
    "i = 0\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    else:\n",
    "        i = i + 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,  5109,  1040,   625,\n",
       "          80,   385,  3475,  1540,  1119,  1362,    33,   170,    94,\n",
       "          46,  2691,  2589,    33,   110,  2090,  1441,   928,  1541,\n",
       "        3943,   957, 13338,   110,    94,    14,   102,   302],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1491, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 760, in fit\n    self._fit(\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 915, in _fit\n    X, y = self._initialize(X, y)\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 852, in _initialize\n    self.model_ = self._build_keras_model()\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 429, in _build_keras_model\n    model = final_build_fn(**build_params)\n  File \"/var/folders/tz/9k5bcs_502z2svd2fr_k2w680000gn/T/ipykernel_927/4019297405.py\", line 66, in create_lstm_model\n    lstm_model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", scoring=\"f1\")\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/keras/src/engine/training.py\", line 3787, in _validate_compile\n    raise TypeError(\nTypeError: Invalid keyword argument(s) in `compile()`: ({'scoring'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\".\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Perform grid search\u001b[39;00m\n\u001b[1;32m     12\u001b[0m grid \u001b[39m=\u001b[39m RandomizedSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_distributions\u001b[39m=\u001b[39mparam_grid,refit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m grid_result \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mfit(X_train, y_train, callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1808\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1810\u001b[0m         ParameterSampler(\n\u001b[1;32m   1811\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1812\u001b[0m         )\n\u001b[1;32m   1813\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    877\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    408\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1491, in fit\n    super().fit(X=X, y=y, sample_weight=sample_weight, **kwargs)\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 760, in fit\n    self._fit(\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 915, in _fit\n    X, y = self._initialize(X, y)\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 852, in _initialize\n    self.model_ = self._build_keras_model()\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/scikeras/wrappers.py\", line 429, in _build_keras_model\n    model = final_build_fn(**build_params)\n  File \"/var/folders/tz/9k5bcs_502z2svd2fr_k2w680000gn/T/ipykernel_927/4019297405.py\", line 66, in create_lstm_model\n    lstm_model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", scoring=\"f1\")\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/opt/anaconda3/envs/tf_env4/lib/python3.8/site-packages/keras/src/engine/training.py\", line 3787, in _validate_compile\n    raise TypeError(\nTypeError: Invalid keyword argument(s) in `compile()`: ({'scoring'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\".\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = KerasClassifier(model=create_lstm_model, units=256, batch_size=64, dropout=0.2, validation_split=0.2,optimizer__learning_rate=0.1, vocab_length=vocab_length, embedding_matrix=embedding_matrix, max_len=avg_len)\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = dict(optimizer__learning_rate=[0.001, 0.01, 0.1],\n",
    "                dropout=[0.1, 0.5], epochs=[20],\n",
    "                units=[64, 128], batch_size=[32, 64])\n",
    "\n",
    "# Perform grid search\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,refit=True,cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.932100 using {'units': 128, 'optimizer__learning_rate': 0.005, 'epochs': 6, 'dropout': 0.5, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#means = grid_result.cv_results_['mean_test_score']\n",
    "#stds = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "    #print(\"%f (%f) wit h: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
       "\tmodel=&lt;function create_lstm_model at 0x107d3e4c0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.2\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=6\n",
       "\tunits=128\n",
       "\tdropout=0.5\n",
       "\toptimizer__learning_rate=0.005\n",
       "\tvocab_length=31463\n",
       "\tembedding_matrix=[[ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]\n",
       " [ 0.13647     0.076255   -0.048948   ...  0.092374   -0.22574\n",
       "  -0.31942999]\n",
       " [ 0.21209    -0.12778001  0.40272    ...  0.25635999 -0.83758003\n",
       "   0.76928997]\n",
       " ...\n",
       " [ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]\n",
       " [-0.18053     0.25543001  0.042019   ...  0.25393     0.09193\n",
       "  -0.57881999]\n",
       " [ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]]\n",
       "\tmax_len=35\n",
       "\tclass_weight=None\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
       "\tmodel=&lt;function create_lstm_model at 0x107d3e4c0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.2\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=6\n",
       "\tunits=128\n",
       "\tdropout=0.5\n",
       "\toptimizer__learning_rate=0.005\n",
       "\tvocab_length=31463\n",
       "\tembedding_matrix=[[ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]\n",
       " [ 0.13647     0.076255   -0.048948   ...  0.092374   -0.22574\n",
       "  -0.31942999]\n",
       " [ 0.21209    -0.12778001  0.40272    ...  0.25635999 -0.83758003\n",
       "   0.76928997]\n",
       " ...\n",
       " [ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]\n",
       " [-0.18053     0.25543001  0.042019   ...  0.25393     0.09193\n",
       "  -0.57881999]\n",
       " [ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]]\n",
       "\tmax_len=35\n",
       "\tclass_weight=None\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasClassifier(\n",
       "\tmodel=<function create_lstm_model at 0x107d3e4c0>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.2\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=6\n",
       "\tunits=128\n",
       "\tdropout=0.5\n",
       "\toptimizer__learning_rate=0.005\n",
       "\tvocab_length=31463\n",
       "\tembedding_matrix=[[ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]\n",
       " [ 0.13647     0.076255   -0.048948   ...  0.092374   -0.22574\n",
       "  -0.31942999]\n",
       " [ 0.21209    -0.12778001  0.40272    ...  0.25635999 -0.83758003\n",
       "   0.76928997]\n",
       " ...\n",
       " [ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]\n",
       " [-0.18053     0.25543001  0.042019   ...  0.25393     0.09193\n",
       "  -0.57881999]\n",
       " [ 0.          0.          0.         ...  0.          0.\n",
       "   0.        ]]\n",
       "\tmax_len=35\n",
       "\tclass_weight=None\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from joblib import dump, load\n",
    "\n",
    "# save model\n",
    "estimator = grid_result.best_estimator_\n",
    "#dump(estimator, \"model_093.joblib\")\n",
    "# Somewhere else\n",
    "#estimator = load(\"your-model.joblib\")\n",
    "estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
