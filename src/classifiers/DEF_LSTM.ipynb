{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, Layer\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import fasttext ###\n",
    "from keras.regularizers import L2\n",
    "import tensorflow.keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../data/New dataset/LSTM/preprocessing/train_tweets_LSTM_pre_new.csv')\n",
    "df_val = pd.read_csv('../../data/New dataset/LSTM/preprocessing/eval_tweets_LSTM_pre_new.csv')\n",
    "df_test = pd.read_csv('../../data/New dataset/LSTM/preprocessing/test_tweets_LSTM_p_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train[\"tweet_text\"], df_train[\"cyberbullying_type\"]\n",
    "X_val, y_val = df_val[\"tweet_text\"], df_val[\"cyberbullying_type\"]\n",
    "X_test, y_test = df_test[\"tweet_text\"], df_test[\"cyberbullying_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tweet_length'] = df_train['tweet_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_train['tweet_length'], bins=30, kde=True)\n",
    "plt.title('Tweet Length distribution')\n",
    "plt.xlabel('Tweet length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_len'] = [len(text.split()) for text in df_train.tweet_text]\n",
    "max_len = 45\n",
    "count = (df_train['text_len'] >= max_len).sum() \n",
    "total_tweets = len(df_train)\n",
    "percentage = (count / total_tweets) * 100\n",
    "print(f\"Percentage of tweets longer than {max_len} tokens: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform label encoder on the target variable\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "#Each word in input used as a key, while a unique index is used as the value of the key \n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)\n",
    "X_val = word_tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"The vocaboulary length is:\", vocab_length)\n",
    "\n",
    "X_train = pad_sequences(X_train, padding = 'pre', maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, padding = 'pre', maxlen = max_len)\n",
    "X_val = pad_sequences(X_val, padding = 'pre', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word embeddings and create a dictionary that willl contain words as keys, and their corresponging embedding list as values. \n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('Embeddings/glove.twitter.27B.50d.txt', encoding=\"utf8\") ## ATTENZIONE: change if you change emebdding dim\n",
    "\n",
    "for line in glove_file:\n",
    "\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 50)) ## ATTENZIONE: change if the dimention of embedding changes above\n",
    "\n",
    "i = 0\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "print(\"The number of out-of-vocabulary words is:\", i)\n",
    "print(\"The percentage of out-of-vocabulary words is:\", (i/vocab_length) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embedding matrix \n",
    "#embedding_matrix = np.load(\"Embeddings/fasttext_embeddingmatrix_no.npy\") \n",
    "\n",
    "#num_words, embedding_dim = embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking everything is working properly\n",
    "\n",
    "print(X_train[1])\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'y_val shape: {y_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - no dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and RandomizedGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=True, #Depending on which embeddings we use\n",
    "                        mask_zero = True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(64, activation='relu')) #regolarizzazione: diminuisci i features prima di generare output\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='baseline_nodense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE layer o NO \n",
    "    project_name='pre_50_rs' ### Inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "As suggested on the Keras official documentation, we retrain the model using the best parameters found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model = build_model(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and RandomizedGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_dense(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=True, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero=True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_dense,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='baseline_dense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='pre_50_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_dense = build_model_dense(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_dense.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_dense.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_dense.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Bidirectional Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional - no dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bi(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=True, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero=True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(Bidirectional(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01))))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "    # RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_bi,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='bidirectional_nodense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='pre_50_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_bi = build_model_bi(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_bi.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_bi.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_bi.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional - dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bi_dense(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=True, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero=True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(Bidirectional(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01))))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "    # RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_bi_dense,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='bidirectional_dense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='pre_50_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_bi_dense = build_model_bi_dense(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_bi_dense.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_bi_dense.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_bi_dense.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape = (sequence_length, features)\n",
    "        # Define the shape of the weights = (features, 1)\n",
    "        self.W = self.add_weight(name='attention_weights',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True)\n",
    "\n",
    "        # Define the shape of the bias (sequence_length, 1) \n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                shape=(input_shape[-2], 1),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # x is the input tensor with shape: (batch_size, sequence_length, features)\n",
    "        # Compute the attention scores, which should have shape (sequence_length, 1)\n",
    "        attention_scores = K.dot(x, self.W)  # Shape: (sequence_length, 1)\n",
    "        # Adding the bias\n",
    "        attention_scores += self.b\n",
    "        # Applying tanh activation to the attention scores\n",
    "        attention_scores = K.tanh(attention_scores)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Use the mask to zero-out padding in the softmax computation\n",
    "            mask = K.cast(mask, K.floatx())  # Convert mask to float\n",
    "            mask = K.expand_dims(mask, axis=-1)  # Make mask shape match attention_scores\n",
    "            attention_scores += (mask - 1) * 1e9  # Apply a large negative to mask-out areas\n",
    "\n",
    "        # Applying softmax to get attention weights across the sequence dimension\n",
    "        attention_weights = K.softmax(attention_scores, axis=1)\n",
    "\n",
    "        # Multiply each value by the corresponding attention weights (broadcasting)\n",
    "        weighted_input = x * attention_weights\n",
    "\n",
    "        # Summing over the sequence length to get the context vector\n",
    "        context_vector = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # The output of the layer is a context vector for each example\n",
    "        # with the dimensionality equal to the feature size of the input.\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Return None because no need to pass the mask to subsequent layers\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention - nodense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_att(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=True, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero = True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01), return_sequences=True))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_att,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='attention_nodense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='pre_50_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)\n",
    "\n",
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_att = build_model_att(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_att.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_att.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_att.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention - dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_att_dense(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=True, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero = True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01), return_sequences=True))\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_att_dense,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='attention_dense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='pre_50_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)\n",
    "\n",
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_att_dense = build_model_att_dense(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_att_dense.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_att_dense.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_att_dense.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cc9b73884fc6dc94cb042d029db4d21e2f9331e7b6a549688c5b94abae7d991"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('ispr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
