{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6406adc-7273-4fbb-9dae-b6ee9e4fc153",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65c6ac",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fff070-230e-47ad-975c-436c0800d7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/g.russo55/Progetto/src/classifiers/../data_preparation/Preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import import_ipynb\n",
    "from data_preparation import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c11e6d-ea4b-4f37-bb7d-d9bc77b7a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b6f2f1-3ee0-4e3c-ab04-4ff181694ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/updated_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196a56f-d3b3-4a12-aa88-bed2620b4943",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a232c0-d63b-4d7a-a5e1-1e6176db611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet_BERT(tweet):\n",
    "    tweet = Preprocessing.remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = Preprocessing.remove_hashtag(tweet)\n",
    "    tweet = Preprocessing.remove_special_characters(tweet)\n",
    " \n",
    "    tweet = Preprocessing.remove_spaces(tweet)\n",
    "    tweet = Preprocessing.remove_textual_emojis(tweet)\n",
    "    tweet = Preprocessing.remove_not_ASCII(tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2104fe-5603-444d-b2e9-f7a44c08505f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tweet_text'] = df['tweet_text'].apply(normalize_tweet_BERT)\n",
    "df = Preprocessing.clean_normalized_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb15ce3-cccc-43ab-81ac-20d91dcd016f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "religion             7963\n",
       "age                  7949\n",
       "ethnicity            7893\n",
       "not_cyberbullying    7711\n",
       "gender               7665\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cyberbullying_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1948bb-5530-49b1-a236-a91c156d182d",
   "metadata": {},
   "source": [
    "In seguito a questo risultato, confermiamo che i dati sono bilanciati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e0ad8-0e47-4732-9677-ab5317586490",
   "metadata": {},
   "source": [
    "### Labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf879a08-3774-4c1f-8d44-5931d715e9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not_cyberbullying': 0, 'gender': 1, 'religion': 2, 'age': 3, 'ethnicity': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.cyberbullying_type.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725ddd8-9b25-4b09-808a-6c03872bb315",
   "metadata": {},
   "source": [
    "Sostituiamo nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cecf9f-88dc-4547-8261-5c42cc9ea66e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761436/349901490.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df.cyberbullying_type.replace(label_dict)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in other words katandandre, your food was crap...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why is aussietv so white?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a classy whore? or more red velvet cupcakes?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  label\n",
       "0  in other words katandandre, your food was crap...  not_cyberbullying      0\n",
       "1                          why is aussietv so white?  not_cyberbullying      0\n",
       "2       a classy whore? or more red velvet cupcakes?  not_cyberbullying      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.cyberbullying_type.replace(label_dict)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654eea2",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9558e8c4-83cf-43cf-b402-4a793c5e8d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">age</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>6750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ethnicity</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gender</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>6505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">not_cyberbullying</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>6528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">religion</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>6778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    tweet_text\n",
       "cyberbullying_type label data_type            \n",
       "age                3     train            6750\n",
       "                         val              1199\n",
       "ethnicity          4     train            6742\n",
       "                         val              1151\n",
       "gender             1     train            6505\n",
       "                         val              1160\n",
       "not_cyberbullying  0     train            6528\n",
       "                         val              1183\n",
       "religion           2     train            6778\n",
       "                         val              1185"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divido il dataset in training e validation set mantenendo la distribuzione delle classi\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=None)\n",
    "\n",
    "# Creare una colonna per segnare il tipo di dato\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "# Assegna 'train' ai dati di addestramento e 'val' ai dati di validazione nel DataFrame\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "# Conta i valori per ogni combinazione di categoria, etichetta e tipo di dato\n",
    "df.groupby(['cyberbullying_type', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee57b7-80cb-4c05-be58-b3304bbc4a34",
   "metadata": {},
   "source": [
    "### BertTokenizer and Encoding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567992b-b170-4430-80ab-ca93d1d15724",
   "metadata": {},
   "source": [
    "**We have to perform _Tokenization_ --> take raw texts and split into tokens, which are numeric data to represent words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941c3709-bfb1-4a68-a006-b1bc086f8310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inizializzazione del tokenizer BERT basato su WordPiece, \n",
    "# instanziando una configurazione bert-base (12 layer) e uncased, dato che durante il preprocessing abbiamo eliminato le lettere maiuscole\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cb96c97-838f-42e0-abb0-cb7ad6cc49fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENCODE_DATA_TYPES = ['train', 'val']\n",
    "MAX_LEN = 256             #CONTROLLARE il valore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a7020f-196e-4068-99a9-34c7e451868b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"encoded_data_train = tokenizer.batch_encode_plus(\\n    df[df.data_type=='train'].tweet_text.values, \\n    add_special_tokens=True,         # Add [CLS] and [SEP] special tokens\\n    return_attention_mask=True,      # it will return the attention mask according to the specific tokenizer defined by the max_length attribute\\n    padding='max_length', \\n    max_length=256,             #CONTROLLARE il valore\\n    truncation=True,\\n    return_tensors='pt'              # return pytorch\\n)\\n\\nencoded_data_val = tokenizer.batch_encode_plus(\\n    df[df.data_type=='val'].tweet_text.values, \\n    add_special_tokens=True, \\n    return_attention_mask=True, \\n    padding='max_length', \\n    max_length=256, \\n    truncation=True,\\n    return_tensors='pt'\\n)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].tweet_text.values, \n",
    "    add_special_tokens=True,         # Add [CLS] and [SEP] special tokens\n",
    "    return_attention_mask=True,      # it will return the attention mask according to the specific tokenizer defined by the max_length attribute\n",
    "    padding='max_length', \n",
    "    max_length=256,             #CONTROLLARE il valore\n",
    "    truncation=True,\n",
    "    return_tensors='pt'              # return pytorch\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].tweet_text.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    padding='max_length', \n",
    "    max_length=256, \n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f86a94e0-5b3b-4060-887e-c27898f5a1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_data(data_type):\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == data_type].tweet_text.values, \n",
    "        add_special_tokens = True,         # Add [CLS] and [SEP] special tokens\n",
    "        return_attention_mask = True,      # it will return the attention mask according to the specific tokenizer defined by the max_length attribute\n",
    "        max_length = MAX_LEN,\n",
    "        padding = 'max_length', \n",
    "        truncation = True,\n",
    "        return_tensors = 'pt'              # return pytorch, i tensori servono a rappresentare e manipolare dati multidimensionali in modo efficiente\n",
    "    )\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9de807-1baf-41eb-b334-12eb8c3c773d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Codifica i dati\n",
    "encoded_data_train = encode_data(ENCODE_DATA_TYPES[0])\n",
    "encoded_data_val = encode_data(ENCODE_DATA_TYPES[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35016b-efd3-46ba-a091-98d6036d8314",
   "metadata": {},
   "source": [
    "Split the data into input_ids, attention_masks and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b7471c-a3c8-47da-817c-6767c5aa7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type == ENCODE_DATA_TYPES[0]].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type == ENCODE_DATA_TYPES[1]].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7d09750-29c1-45d4-8b97-f7f378bbf5df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if encoded_data_train2.equal(encoded_data_train):\\n    print('OKAY')\\nelse:\\n    print('NO')\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''if encoded_data_train2.equal(encoded_data_train):\n",
    "    print('OKAY')\n",
    "else:\n",
    "    print('NO')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318cd30-e42a-4afb-b36a-2e80ef9a34bf",
   "metadata": {},
   "source": [
    "Finally, after we get encoded data set, we can create training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "093d0444-05c0-4770-92f7-bc9f4527fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset consente di creare un dataset basato su tensori, \n",
    "# utile soprattutto quando si lavora con dati che possono essere rappresentati come tensori\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f280a1e2-cae6-496b-8710-08d0e38281c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33303, 5878)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c903d-9163-4e65-a067-584153de1c99",
   "metadata": {},
   "source": [
    "### BERT Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e135e7-6518-4222-95d0-163d65beb6c8",
   "metadata": {},
   "source": [
    "_bert-base-uncased_ is a smaller pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f894cbe-47a6-4362-8e89-fd250c828f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "# gli ultimi due non sono necessari + settando a False riduciamo il peso computazionale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d359a25-c177-4251-bf0d-5b9357a02c93",
   "metadata": {},
   "source": [
    "- DataLoader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "- We use RandomSampler for training and SequentialSampler for validation.\n",
    "- Given the limited memory in my environment, I set batch_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d4d5b3d-cf57-4bbd-912e-80707ea20ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e88f1-474f-499a-a496-bfcd0462e3d7",
   "metadata": {},
   "source": [
    "##### Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8cf845-9cf4-449b-b886-b2de938d789f",
   "metadata": {},
   "source": [
    "- To construct an optimizer, we have to give it an iterable containing the parameters to optimize. Then, we can specify optimizer-specific options such as the learning rate, epsilon, etc\n",
    "- Search for epochs=X which works well for this dataset\n",
    "- Create a schedule with a learning rate that decreases linearly from the initial learning rate set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial learning rate set in the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f07b45b-024a-4d8b-936b-fc39d76edf2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g.russo55/HLT/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,   #learning rate\n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 3 # CONTROLLARE VALORE (5)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ea6a7-66ad-4026-ac17-af3410cffcc1",
   "metadata": {},
   "source": [
    "**nota**: epsilon è un piccolo valore aggiunto al denominatore per evitare la divisione per zero o instabilità numerica durante il training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8c5c5-0ad2-445d-bf6a-1814d1784831",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ba33c-0710-42b1-9612-8d62f268614f",
   "metadata": {},
   "source": [
    "We will use f1 score and accuracy per class as performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff1aa890-f7b0-47a4-987d-d343c087e5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21b12cfe-96bd-4fb1-8d7f-190476b4ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_accuracy(predictions, true_vals):\n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    return accuracy_score(labels_flat, preds_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c56568-7943-4bfa-9c50-42a8ffcee100",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "286eebd4-ba66-4453-b034-3d5d5273aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0acdaf9-b521-4952-be05-95338365ef99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a73e3f2-bc96-444a-9197-262ab4ff347d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "592fc221-4039-440f-a8bf-8749c8068847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d0494fe-1780-4438-a34b-3cf334fb0c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0baf161094a473eb092e585bdd559f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Average Training loss: 0.32646875974370054\n",
      "Validation loss: 0.1982333490361824\n",
      "F1 Score (Weighted): 0.933089944182388\n",
      "Validation Accuracy: 0.93297039809459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Average Training loss: 0.1465318143839685\n",
      "Validation loss: 0.17283179407873514\n",
      "F1 Score (Weighted): 0.9438388515838645\n",
      "Validation Accuracy: 0.9436883293637292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Average Training loss: 0.10174632099887888\n",
      "Validation loss: 0.18258128317614572\n",
      "F1 Score (Weighted): 0.9426537458830463\n",
      "Validation Accuracy: 0.9424974481116026\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Average Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    val_accuracy = calculate_accuracy(predictions, true_vals)  # Calcolo dell'accuratezza\n",
    "        \n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "    tqdm.write(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "torch.save(model.state_dict(), f'../../data/BERT/finetuned_BERT.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83681dec-2aa3-4ed5-b497-4e9e678b694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: not_cyberbullying\n",
      "Accuracy: 1046/1183\n",
      "\n",
      "Class: gender\n",
      "Accuracy: 1043/1160\n",
      "\n",
      "Class: religion\n",
      "Accuracy: 1144/1185\n",
      "\n",
      "Class: age\n",
      "Accuracy: 1175/1199\n",
      "\n",
      "Class: ethnicity\n",
      "Accuracy: 1132/1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../../data/BERT/finetuned_BERT.model', map_location=device))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7628d1d-13ec-4ff8-89d5-bc0d73f2d41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1046   69   45   12   11]\n",
      " [ 110 1043    5    1    1]\n",
      " [  33    4 1144    1    3]\n",
      " [  20    4    0 1175    0]\n",
      " [  10    4    4    1 1132]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      1183\n",
      "           1       0.93      0.90      0.91      1160\n",
      "           2       0.95      0.97      0.96      1185\n",
      "           3       0.99      0.98      0.98      1199\n",
      "           4       0.99      0.98      0.99      1151\n",
      "\n",
      "    accuracy                           0.94      5878\n",
      "   macro avg       0.94      0.94      0.94      5878\n",
      "weighted avg       0.94      0.94      0.94      5878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix and Classification Report with checks\n",
    "\n",
    "preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix(labels_flat, preds_flat))\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(labels_flat, preds_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bca08-996b-4367-a422-98a3a79f66e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
