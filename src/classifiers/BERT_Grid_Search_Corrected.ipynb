{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64db8417",
   "metadata": {},
   "source": [
    "# Grid Search for BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f917147c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/g.russo55/Progetto/src/classifiers/../data_preparation/Preprocessing.ipynb\n",
      "{'not_cyberbullying': 0, 'gender': 1, 'religion': 2, 'age': 3, 'ethnicity': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_751437/528982114.py:41: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df.cyberbullying_type.replace(label_dict)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid Search...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823dd879b637441ab2e06c48c03d300d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grid Search Progress:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I migliori parametri trovati sono: {'learning_rate': 2e-05, 'batch_size': 64, 'num_train_epochs': 3} con un punteggio di: 0.9443613652357495\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import import_ipynb\n",
    "from data_preparation import Preprocessing\n",
    "\n",
    "# Caricamento e preparazione dei dati\n",
    "df = pd.read_csv('../../data/updated_tweets.csv')\n",
    "\n",
    "def normalize_tweet_BERT(tweet):\n",
    "    tweet = Preprocessing.remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = Preprocessing.remove_hashtag(tweet)\n",
    "    tweet = Preprocessing.remove_special_characters(tweet)\n",
    " \n",
    "    tweet = Preprocessing.remove_spaces(tweet)\n",
    "    tweet = Preprocessing.remove_textual_emojis(tweet)\n",
    "    tweet = Preprocessing.remove_not_ASCII(tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "df['tweet_text'] = df['tweet_text'].apply(normalize_tweet_BERT)\n",
    "df = Preprocessing.clean_normalized_df(df)\n",
    "\n",
    "\n",
    "possible_labels = df.cyberbullying_type.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "print(label_dict)\n",
    "df['label'] = df.cyberbullying_type.replace(label_dict)\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for tweet in df.tweet_text:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            tweet,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = 64,\n",
    "                            padding = 'max_length', \n",
    "                            return_attention_mask = True,\n",
    "                            truncation = True,\n",
    "                            return_tensors = 'pt',\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(df.label.values)\n",
    "    \n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "input_ids, attention_masks, labels = preprocess_data(df)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, epochs, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_masks, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_accuracy = 0\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        val_accuracy += accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "    return val_accuracy / len(val_dataloader)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-5, 2e-5, 3e-5],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'num_train_epochs': [2, 3, 4]\n",
    "}\n",
    "\n",
    "'''def grid_search(param_grid, model, train_dataloader, val_dataloader, device):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    for lr in param_grid['learning_rate']:\n",
    "        for bs in param_grid['batch_size']:\n",
    "            for epochs in param_grid['num_train_epochs']:\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "                score = train_model(model, train_dataloader, val_dataloader, epochs, optimizer, device)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'learning_rate': lr, 'batch_size': bs, 'num_train_epochs': epochs}\n",
    "    return best_params, best_score\n",
    "'''\n",
    "def grid_search(param_grid, model, train_dataloader, val_dataloader, device):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "    total_combinations = len(param_grid['learning_rate']) * len(param_grid['batch_size']) * len(param_grid['num_train_epochs'])\n",
    "    with tqdm(total=total_combinations, desc=\"Grid Search Progress\") as pbar:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for bs in param_grid['batch_size']:\n",
    "                for epochs in param_grid['num_train_epochs']:\n",
    "                    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "                    score = train_model(model, train_dataloader, val_dataloader, epochs, optimizer, device)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {'learning_rate': lr, 'batch_size': bs, 'num_train_epochs': epochs}\n",
    "                    pbar.update(1)\n",
    "    return best_params, best_score\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"\\nGrid Search...\")\n",
    "best_params, best_score = grid_search(param_grid, model, train_dataloader, val_dataloader, device)\n",
    "\n",
    "print(f'I migliori parametri trovati sono: {best_params} con un punteggio di: {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba7827-5c41-4a35-b613-e08c9f2dc7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
