{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6406adc-7273-4fbb-9dae-b6ee9e4fc153",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6b2a0",
   "metadata": {},
   "source": [
    "- bert-base-uncased (model_1)\n",
    "- bert-large-whole-word-masking (model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0925a",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56c11e6d-ea4b-4f37-bb7d-d9bc77b7a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "#from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7704334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "\n",
    "# Dizionario per bert-base-uncased\n",
    "model_1_info = {\n",
    "    \"NAME\": \"bert-base-uncased\",\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"EPOCHS\": 3,\n",
    "    \"LR\": 2e-5\n",
    "}\n",
    "\n",
    "# Dizionario per bert-large-whole-word-masking\n",
    "model_2_info = {\n",
    "    \"NAME\": \"bert-large-whole-word-masking\",\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"EPOCHS\": 4,\n",
    "    \"LR\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5577ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529af9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install iProgress\n",
    "#pip install ipywidgets\n",
    "#!pip install ipywidgets --upgrade\n",
    "#!pip install ipywidgets tqdm --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f65c6ac",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b6f2f1-3ee0-4e3c-ab04-4ff181694ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data/train_tweets_Transformers.csv')\n",
    "eval_df = pd.read_csv('../../data/eval_tweets_Transformers.csv')\n",
    "test_df = pd.read_csv('../../data/test_tweets_Transformers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e0ad8-0e47-4732-9677-ab5317586490",
   "metadata": {},
   "source": [
    "### Labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf879a08-3774-4c1f-8d44-5931d715e9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ethnicity': 0, 'age': 1, 'gender': 2, 'not_cyberbullying': 3, 'religion': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''possible_labels = train_df.cyberbullying_type.unique()\n",
    "\n",
    "le = LabelEncoder()\n",
    "label_dict = le.fit_transform(possible_labels)\n",
    "\n",
    "print(label_dict)\n",
    "label_dict_inverse = le.inverse_transform(possible_labels)\n",
    "label_dict_inverse'''\n",
    "\n",
    "possible_labels = train_df.cyberbullying_type.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725ddd8-9b25-4b09-808a-6c03872bb315",
   "metadata": {},
   "source": [
    "Sostituiamo nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95cecf9f-88dc-4547-8261-5c42cc9ea66e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2885/2304191651.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df['label'] = train_df.cyberbullying_type.replace(label_dict)\n",
      "/tmp/ipykernel_2885/2304191651.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  eval_df['label'] = eval_df.cyberbullying_type.replace(label_dict)\n",
      "/tmp/ipykernel_2885/2304191651.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df['label'] = test_df.cyberbullying_type.replace(label_dict)\n"
     ]
    }
   ],
   "source": [
    "'''train_df['label'] = le.fit_transform(train_df['cyberbullying_type'])\n",
    "eval_df['label'] = le.fit_transform(eval_df['cyberbullying_type'])\n",
    "test_df['label'] = le.fit_transform(test_df['cyberbullying_type'])'''\n",
    "\n",
    "train_df['label'] = train_df.cyberbullying_type.replace(label_dict)\n",
    "eval_df['label'] = eval_df.cyberbullying_type.replace(label_dict)\n",
    "test_df['label'] = test_df.cyberbullying_type.replace(label_dict)\n",
    "\n",
    "\n",
    "train_dff = train_df\n",
    "train_df = train_df[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f86a94e0-5b3b-4060-887e-c27898f5a1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_data(df, tokenizer):\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        df.tweet_text.values, \n",
    "        add_special_tokens = True,         # Add [CLS] and [SEP] special tokens\n",
    "        return_attention_mask = True,      # it will return the attention mask according to the specific tokenizer defined by the max_length attribute\n",
    "        max_length = MAX_LEN,\n",
    "        padding = 'max_length', \n",
    "        truncation = True,\n",
    "        return_tensors = 'pt'              # return pytorch, i tensori servono a rappresentare e manipolare dati multidimensionali in modo efficiente\n",
    "    )\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e44888af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(tokenizer, batch_size):\n",
    "    # Codifica i dati\n",
    "    encoded_data_train = encode_data(train_df, tokenizer)\n",
    "    encoded_data_val = encode_data(eval_df, tokenizer)\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(train_df.label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(eval_df.label.values) #convertire le etichette in tensori\n",
    "\n",
    "    # TensorDataset consente di creare un dataset basato su tensori, \n",
    "    # utile soprattutto quando si lavora con dati che possono essere rappresentati come tensori\n",
    "\n",
    "    #Combines the input IDs, attention masks, and labels for the training set into a TensorDataset.\n",
    "    # This allows the data to be easily accessed and used by PyTorch's DataLoader.\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "    dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "    return dataloader_train, dataloader_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1ba33c-0710-42b1-9612-8d62f268614f",
   "metadata": {},
   "source": [
    "We will use f1 score and accuracy per class as performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff1aa890-f7b0-47a4-987d-d343c087e5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "def calculate_accuracy(predictions, true_vals):\n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "    return accuracy_score(labels_flat, preds_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "592fc221-4039-440f-a8bf-8749c8068847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val, model):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d0494fe-1780-4438-a34b-3cf334fb0c92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def show_plots(history):\\n    # Plot training & validation loss values\\n    plt.figure(figsize=(12, 4))\\n\\n    plt.subplot(1, 2, 1)\\n    plt.plot(history['train_loss'], label='Training Loss')\\n    plt.plot(history['val_loss'], label='Validation Loss')\\n    plt.title('Loss')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('Loss')\\n    plt.legend()\\n\\n    # Plot training & validation accuracy values\\n    plt.subplot(1, 2, 2)\\n    plt.plot(history['train_acc'], label='Training Accuracy')\\n    plt.plot(history['val_acc'], label='Validation Accuracy')\\n    plt.title('Accuracy')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('Accuracy')\\n    plt.legend()\\n\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(model, dataloader_train, dataloader_validation, \n",
    "                epochs, optimizer, scheduler):\n",
    "    \n",
    "    # Initialize history dictionary\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        correct_train, total_train = 0, 0\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':    batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[2].long(),\n",
    "                    }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "            \n",
    "            _, preds = torch.max(outputs[1], dim=1)\n",
    "            correct_train += torch.sum(preds == inputs['labels'])\n",
    "            total_train += len(inputs['labels'])\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "        train_acc = correct_train.double() / total_train\n",
    "        tqdm.write(f'Average Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals = evaluate(dataloader_validation, model)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        val_accuracy = calculate_accuracy(predictions, true_vals)\n",
    "            \n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "        # Append metrics to history\n",
    "        history['train_loss'].append(loss_train_avg)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "\n",
    "    model_name = model.config.name_or_path\n",
    "    torch.save(model.state_dict(), f'../../data/Transformers/finetuned_{model_name}.model')\n",
    "    \n",
    "    return history\n",
    "\n",
    "'''def show_plots(history):\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d9bca08-996b-4367-a422-98a3a79f66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(model_info):\n",
    "    name = model_info[\"NAME\"]\n",
    "    batch_size = model_info[\"BATCH_SIZE\"]\n",
    "    epochs = model_info[\"EPOCHS\"]\n",
    "    lr = model_info[\"LR\"]\n",
    "\n",
    "    # Inizializzazione del tokenizer BERT basato su WordPiece, \n",
    "    # instanziando una configurazione bert-base (12 layer) e uncased, dato che durante il preprocessing abbiamo eliminato le lettere maiuscole\n",
    "    tokenizer = BertTokenizer.from_pretrained(name, do_lower_case=True)\n",
    "\n",
    "    dataloader_train, dataloader_validation = get_dataloaders(tokenizer, batch_size)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(name,\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "    # gli ultimi due non sono necessari + settando a False riduciamo il peso computazionale\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr) #, eps=1e-8\n",
    "                  \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"\\n*** Avvio TRAINING ***\")\n",
    "\n",
    "    history = train_model(model, dataloader_train, dataloader_validation, epochs, optimizer, scheduler) #\n",
    "    print(\"\\n*** Fine TRAINING ***\")\n",
    "    print(\"\\n -------- \\n\")\n",
    "\n",
    "    _, predictions, true_vals = evaluate(dataloader_validation, model)\n",
    "    accuracy_per_class(predictions, true_vals)\n",
    "\n",
    "    preds_flat = np.argmax(predictions, axis=1).flatten()\n",
    "    labels_flat = true_vals.flatten()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(confusion_matrix(labels_flat, preds_flat))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(classification_report(labels_flat, preds_flat))\n",
    "\n",
    "    return model, tokenizer, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "852a83fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Avvio TRAINING ***\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab72f4fc17644e7696512b6939054c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c926cc116441ea949124f91d102677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Average Training loss: 1.5312456786632538\n"
     ]
    }
   ],
   "source": [
    "model_1, tokenizer_1, history_1 = execute(model_1_info)\n",
    "#model_2, tokenizer_2 = execute(model_2_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_plots(history):\n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438c6bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m show_plots(\u001b[43mhistory_1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history_1' is not defined"
     ]
    }
   ],
   "source": [
    "show_plots(history_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fdaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "        'train_loss': [0.3620856624607205, 0.1482452382688141, 0.10468212381267665],\n",
    "        'val_loss': [0.22305859933898906, 0.2112742636089373, 0.21650002296600077],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [0.9263241106719368, 0.9302766798418972, 0.9282213438735177]\n",
    "    }\n",
    "show_plots(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07897c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2, tokenizer_2, history_2 = execute(model_2_info)\n",
    "show_plots(history_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aeda6e",
   "metadata": {},
   "source": [
    "## Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader_test):\n",
    "    model.eval()\n",
    "    predictions, true_labels = []\n",
    "\n",
    "    for batch in dataloader_test:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        logits = torch.argmax(logits, dim=1).flatten().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        \n",
    "        predictions.extend(logits)\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "\n",
    "# Codifica i dati di test e crea un dataloader\n",
    "tokenizer = BertTokenizer.from_pretrained(model_1_info[\"NAME\"])\n",
    "encoded_data_test = encode_data(test_df, tokenizer)\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(test_df.label.values)\n",
    "\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "dataloader_test = DataLoader(dataset_test, sampler=SequentialSampler(dataset_test), batch_size=model_1_info[\"BATCH_SIZE\"])\n",
    "\n",
    "# Esegui la valutazione del modello\n",
    "#model = BertForSequenceClassification.from_pretrained(model_1_info[\"NAME\"], num_labels=len(label_dict))\n",
    "#model.to(device)\n",
    "evaluate_model(model_1, dataloader_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc8d8a",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c949224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Style\n",
    "\n",
    "def predict(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_dict['input_ids'].to(device)\n",
    "    attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    prediction_type = next((chiave for chiave, valore in label_dict.items() \n",
    "                           if valore == prediction), None)\n",
    "    return prediction_type\n",
    "    #return invert_label(prediction)  \n",
    "\n",
    "#TODO: aggiungere normalizzazione al tweet in ingresso   \n",
    "def print_category(sentence):\n",
    "    predicted_category = predict(sentence, model_1, tokenizer_1, device)\n",
    "    print(f\"Text: {sentence} \\nPredicted Cyberbullying Category: \", end=\"\")\n",
    "    print(Fore.BLUE + Style.BRIGHT + f\"{predicted_category}\"+ Style.RESET_ALL)\n",
    "    print(\"----------------\")\n",
    "    return\n",
    "    \n",
    "print_category(\"Example of a new tweet that could be cyberbullying.\")\n",
    "print_category(\"fuck you black\")\n",
    "print_category(\"i will rape you\")\n",
    "print_category(\"muslim idiot\")\n",
    "print_category(\"muslim idiot\")\n",
    "print_category(\"hello how are you\")\n",
    "print_category(\"Can anyone else said to this nigger that the dress is blue?\")\n",
    "print_category(\"I'm really happy for your birthday\")\n",
    "print_category(\"In my opinion Allah is not a real god\")\n",
    "print_category(\"I fucking hate Allah\")\n",
    "print_category(\"I appreciate Allah\")\n",
    "print_category(\"Men are better than women\")\n",
    "print_category(\"Bro, you are a Nigga!!!\")\n",
    "print_category(\"You are a shit!!!\")\n",
    "print_category(\"dickhead!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
