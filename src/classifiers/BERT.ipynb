{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6406adc-7273-4fbb-9dae-b6ee9e4fc153",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4f86cc9-d7f4-4b71-9e19-30935763d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90fff070-230e-47ad-975c-436c0800d7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in alternativa usare %%writefile per creare un file py e importare questo\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import import_ipynb\n",
    "from data_preparation import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33b6f2f1-3ee0-4e3c-ab04-4ff181694ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/updated_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc731a8-67ef-4f4a-8235-fda2fb33f8f3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196a56f-d3b3-4a12-aa88-bed2620b4943",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66a232c0-d63b-4d7a-a5e1-1e6176db611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet_BERT(tweet):\n",
    "    tweet = Preprocessing.remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = Preprocessing.remove_hashtag(tweet)\n",
    "    tweet = Preprocessing.remove_special_characters(tweet)\n",
    " \n",
    "    tweet = Preprocessing.remove_spaces(tweet)\n",
    "    tweet = Preprocessing.remove_textual_emojis(tweet)\n",
    "    tweet = Preprocessing.remove_not_ASCII(tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f2104fe-5603-444d-b2e9-f7a44c08505f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.tweet_text = [normalize_tweet_BERT(tweet) for tweet in df.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7da51853-2f64-4288-aae6-5fcc478fa1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = Preprocessing.clean_normalized_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654eea2",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c58439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorizer = TfidfVectorizer(max_features=10000)  # Puoi regolare il numero massimo di features\\nX_train_vectorized = vectorizer.fit_transform(X_train)\\nX_test_vectorized = vectorizer.transform(X_test)\\n\\n# Addestramento del modello su X_train_vectorized e y_train\\nmodel = LogisticRegression()\\nmodel.fit(X_train_vectorized, y_train)\\n\\n# Effettua le previsioni sul set di test\\ny_pred = model.predict(X_test_vectorized)\\n\\n# Calcola le metriche di valutazione\\naccuracy = accuracy_score(y_test, y_pred)\\nprecision = precision_score(y_test, y_pred, average=\\'weighted\\')\\nrecall = recall_score(y_test, y_pred, average=\\'weighted\\')\\nf1 = f1_score(y_test, y_pred, average=\\'weighted\\')\\n\\n# Stampa le metriche di valutazione\\nprint(\"Accuracy:\", accuracy)\\nprint(\"Precision:\", precision)\\nprint(\"Recall:\", recall)\\nprint(\"F1 Score:\", f1) \\nX_train.head()'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split del DataFrame in X (features) e y (etichette)\n",
    "X = df['tweet_text']\n",
    "y = df['cyberbullying_type']\n",
    "\n",
    "# Split stratificato del dataset in set di addestramento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Esempio di vettorizzazione delle features dei tweet utilizzando TF-IDF\n",
    "'''vectorizer = TfidfVectorizer(max_features=10000)  # Puoi regolare il numero massimo di features\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Addestramento del modello su X_train_vectorized e y_train\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Effettua le previsioni sul set di test\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Calcola le metriche di valutazione\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Stampa le metriche di valutazione\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1) \n",
    "X_train.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fbeef-694e-4e58-8404-366f75d20ac4",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "754ce9c8-8870-48e7-823f-e0b6c3bfb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)           #cambiare nome bert-base-uncased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "24be812f-564c-4c65-bf6f-a5d171671ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df.tweet_text] \\nmax_len = max([len(t) for t in encoded_tweets])\\nmax_len'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in df.tweet_text] \n",
    "max_len = max([len(t) for t in encoded_tweets])\n",
    "max_len'''\n",
    "#la differenza è che tokenizer.encode restituisce una sequenza di interi codificati che rappresentano i token,\n",
    "#includendo anche l'aggiunta di token speciali e altre operazioni di preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32119d02-99d5-4ef5-965f-58fb3dd41174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer.tokenize(tweet) for tweet in df.tweet_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e27f8e-db11-4372-aebc-3d3f3db45912",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1a28b-b3ae-4d98-86c3-8627980e7bcb",
   "metadata": {},
   "source": [
    "Cerchiamo la frase con più token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c03f5379-e4a4-4900-b947-54acce7d6488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(t) for t in tokens])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b625f7c5-6ee9-4427-9fb3-5465f969425a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.62096424287282"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lunghezza_media(tokens):\n",
    "    return sum(len(array) for array in tokens) / len(tokens)\n",
    "\n",
    "avg_len_tweet = lunghezza_media(tokens)\n",
    "avg_len_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39979936-36fe-4329-975b-d6004071b931",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for t in tokens:\\n    if len(t) == 428:\\n        print(t)'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for t in tokens:\n",
    "    if len(t) == 428:\n",
    "        print(t)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6abddab4-3f83-4d74-a26c-4bc56097b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e01c5bd4-a7fd-41fc-9137-1c78e7ad8828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLS', 'why', 'is', 'aus', '##sie', '##tv', 'so', 'white', '?', 'SEP']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens)):\n",
    "    t = tokens[i][:max_length - 2]  # -2 per fare spazio ai token speciali [CLS] e [SEP]\n",
    "    # Aggiungi token speciali\n",
    "    t.insert(0, 'CLS')\n",
    "    t.append('SEP')\n",
    "    tokens[i] = t\n",
    "\n",
    "print(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "024e88ef-0a7b-478e-a74f-b3c825502f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokens = [['CLS'] + t[:max_length - 2] + ['SEP'] for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca98d44-2be0-484a-a0e5-eaab70d811c0",
   "metadata": {},
   "source": [
    "### Creazione della maschera di attenzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e0be1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLS', 'why', 'is', 'aus', '##sie', '##tv', 'so', 'white', '?', 'SEP', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['CLS', 'a', 'class', '##y', 'whore', '?', 'or', 'more', 'red', 'velvet', 'cup', '##cake', '##s', '?', 'SEP', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Input IDs: tensor([[  100,  1999,  2060,  ...,     0,     0,     0],\n",
      "        [  100,  2339,  2003,  ...,     0,     0,     0],\n",
      "        [  100,  1037,  2465,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  100,  1045,  8415,  ...,     0,     0,     0],\n",
      "        [  100,  6300,  2050,  ...,     0,     0,     0],\n",
      "        [  100, 22953,  1012,  ...,     0,     0,     0]])\n",
      "Attention Mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Tokenizzazione e padding\n",
    "padded_tokens = [t + ['[PAD]'] * (max_length - len(t)) for t in tokens]\n",
    "\n",
    "# Creazione della maschera di attenzione\n",
    "attention_mask = [[1] * len(t) + [0] * (max_length - len(t)) for t in tokens]\n",
    "\n",
    "# Converti in tensori PyTorch\n",
    "input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(t) for t in padded_tokens])\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "# Stampa l'input preparato\n",
    "print(padded_tokens[1])\n",
    "print(padded_tokens[2])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "de787cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import torch\\nfrom transformers import BertModel\\n\\n# Carica il modello pre-addestrato BERT\\nmodel = BertModel.from_pretrained(\\'bert-base-uncased\\')\\n\\n# Passa l\\'input attraverso il modello BERT per ottenere le rappresentazioni nascoste\\noutputs = model(input_ids, attention_mask=attention_mask)\\n\\n# Estrai le rappresentazioni nascoste dell\\'ultimo layer\\nhidden_states = outputs.last_hidden_state\\n\\n# Stampa le dimensioni delle rappresentazioni nascoste\\nprint(\"Dimensioni delle rappresentazioni nascoste:\", hidden_states.size())'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "# Carica il modello pre-addestrato BERT\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Passa l'input attraverso il modello BERT per ottenere le rappresentazioni nascoste\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Estrai le rappresentazioni nascoste dell'ultimo layer\n",
    "hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Stampa le dimensioni delle rappresentazioni nascoste\n",
    "print(\"Dimensioni delle rappresentazioni nascoste:\", hidden_states.size())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c01c390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
