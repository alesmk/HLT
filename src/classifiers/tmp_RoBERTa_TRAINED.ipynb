{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6406adc-7273-4fbb-9dae-b6ee9e4fc153",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fff070-230e-47ad-975c-436c0800d7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /Users/alessia/UniProjects/HLT/src/classifiers/../data_preparation/Preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import import_ipynb\n",
    "from data_preparation import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c11e6d-ea4b-4f37-bb7d-d9bc77b7a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformers library for BERT\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import time\n",
    "\n",
    "# Set seed for reproducibility\n",
    "import random\n",
    "seed_value = 2042\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b6f2f1-3ee0-4e3c-ab04-4ff181694ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/updated_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196a56f-d3b3-4a12-aa88-bed2620b4943",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a232c0-d63b-4d7a-a5e1-1e6176db611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tweet_BERT(tweet):\n",
    "    tweet = Preprocessing.remove_links_mentions(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = Preprocessing.remove_hashtag(tweet)\n",
    "    tweet = Preprocessing.remove_special_characters(tweet)\n",
    " \n",
    "    tweet = Preprocessing.remove_spaces(tweet)\n",
    "    tweet = Preprocessing.remove_textual_emojis(tweet)\n",
    "    tweet = Preprocessing.remove_not_ASCII(tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2104fe-5603-444d-b2e9-f7a44c08505f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tweet_text'] = df['tweet_text'].apply(normalize_tweet_BERT)\n",
    "df = Preprocessing.clean_normalized_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afb15ce3-cccc-43ab-81ac-20d91dcd016f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "religion             7963\n",
       "age                  7949\n",
       "ethnicity            7893\n",
       "not_cyberbullying    7711\n",
       "gender               7665\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cyberbullying_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1948bb-5530-49b1-a236-a91c156d182d",
   "metadata": {},
   "source": [
    "In seguito a questo risultato, confermiamo che i dati sono bilanciati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e0ad8-0e47-4732-9677-ab5317586490",
   "metadata": {},
   "source": [
    "### Labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf879a08-3774-4c1f-8d44-5931d715e9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not_cyberbullying': 0, 'gender': 1, 'religion': 2, 'age': 3, 'ethnicity': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.cyberbullying_type.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725ddd8-9b25-4b09-808a-6c03872bb315",
   "metadata": {},
   "source": [
    "Sostituiamo nel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95cecf9f-88dc-4547-8261-5c42cc9ea66e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in other words katandandre, your food was crap...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why is aussietv so white?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a classy whore? or more red velvet cupcakes?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  label\n",
       "0  in other words katandandre, your food was crap...  not_cyberbullying      0\n",
       "1                          why is aussietv so white?  not_cyberbullying      0\n",
       "2       a classy whore? or more red velvet cupcakes?  not_cyberbullying      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.cyberbullying_type.replace(label_dict)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654eea2",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9558e8c4-83cf-43cf-b402-4a793c5e8d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['tweet_text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee57b7-80cb-4c05-be58-b3304bbc4a34",
   "metadata": {},
   "source": [
    "### RoBERTa Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941c3709-bfb1-4a68-a006-b1bc086f8310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name, truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2e5cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lunghezza media dei tweet è: 135.88479109772595\n"
     ]
    }
   ],
   "source": [
    "def calcola_media_lunghezza_tweet(tweets):\n",
    "    \"\"\"\n",
    "    Calcola la lunghezza media dei tweet in una lista.\n",
    "\n",
    "    Args:\n",
    "    tweets (list of str): Lista di tweet.\n",
    "\n",
    "    Returns:\n",
    "    float: Lunghezza media dei tweet.\n",
    "    \"\"\"\n",
    "    if not tweets:\n",
    "        return 0.0\n",
    "\n",
    "    lunghezze = [len(tweet) for tweet in tweets]\n",
    "    media_lunghezza = sum(lunghezze) / len(lunghezze)\n",
    "    return media_lunghezza\n",
    "\n",
    "tweets = df['tweet_text'].tolist()\n",
    "media_lunghezza = calcola_media_lunghezza_tweet(tweets)\n",
    "print(f\"La lunghezza media dei tweet è: {media_lunghezza}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4443727-24e1-40e9-aafb-ac19f24846c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e389956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93cb196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f86a94e0-5b3b-4060-887e-c27898f5a1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  415\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "'''def encode_data(data_type):\n",
    "    encoded_data = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type == data_type].tweet_text.values, \n",
    "        add_special_tokens = True,         # Add [CLS] and [SEP] special tokens\n",
    "        return_attention_mask = True,      # it will return the attention mask according to the specific tokenizer defined by the max_length attribute\n",
    "        max_length = MAX_LEN,\n",
    "        padding = 'max_length', \n",
    "        truncation = True,\n",
    "        return_tensors = 'pt'              # return pytorch, i tensori servono a rappresentare e manipolare dati multidimensionali in modo efficiente\n",
    "    )\n",
    "    return encoded_data\n",
    "'''\n",
    "\n",
    "def roberta_tokenizer(data, max_len=MAX_LEN):\n",
    "    print(MAX_LEN)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,        # Add [CLS] and [SEP] special tokens\n",
    "            max_length=max_len,             # Choose max length to truncate/pad\n",
    "            padding = 'max_length', \n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = pad_sequences(attention_masks, maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Tokenize train tweets\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]\n",
    "\n",
    "# Find the longest tokenized tweet\n",
    "maxx_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', maxx_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b995a0-8a2e-414b-84b6-2c8ef8126e6c",
   "metadata": {},
   "source": [
    "Tokenize the train, validation and test tweets using the custom define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9272843-4c82-47f0-87f2-c81c2dc361f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = roberta_tokenizer(X_train)\n",
    "val_inputs, val_masks = roberta_tokenizer(X_valid)\n",
    "test_inputs, test_masks = roberta_tokenizer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6c06bb-4e06-436b-923a-a4a671c95f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_inputs[1])\n",
    "#train_masks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bab98-e703-4889-93ef-da55a21f5efc",
   "metadata": {},
   "source": [
    "Since we are using the BERT model built on PyTorch, we need to convert the arrays to pytorch tensors and create *dataloaders* for the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e21dd865-9e0b-4b6d-a3cf-1b5e16ae7c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 3,  ..., 1, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert target columns to pytorch tensors format\n",
    "train_labels = torch.from_numpy(y_train)\n",
    "val_labels = torch.from_numpy(y_valid)\n",
    "test_labels = torch.from_numpy(y_test)\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035262d6-0fc3-40e0-929a-66f55acb0f49",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a859dac-9f77-4bac-aa4a-56d1813748a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f67249-dabf-478e-8ddd-eba0ecc86e69",
   "metadata": {},
   "source": [
    "#### ROBERTA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001eb01a-0a3a-4709-9cef-caabe454923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#La classe Roberta_Classifier eredita da nn.Module e rappresenta \n",
    "# il modello di classificazione basato su RoBERTa\n",
    "\n",
    "class Roberta_Classifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(Roberta_Classifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n",
    "        n_input = 768\n",
    "        n_hidden = 50\n",
    "        n_output = 6\n",
    "\n",
    "        # Instantiate BERT model (pre-trained)\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "        # Instantiate the classifier (a fully connected layer followed by a ReLU activation and another fully connected layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),   #Un classificatore sequenziale con un livello lineare, dropout, attivazione LeakyReLU, e un altro livello lineare\n",
    "            nn.Dropout(0.2),    #definisce un livello di dropout con una probabilità di 0.3 per prevenire l'overfitting\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(n_hidden, n_output)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model weights if freeze_bert is True (useful for feature extraction without fine-tuning)\n",
    "        if freeze_bert:\n",
    "            for param in self.roberta.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input data (input_ids and attention_mask) to BERT\n",
    "        outputs = self.roberta(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the `[CLS]` token from the BERT output (useful for classification tasks)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed the extracted hidden state to the classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "#Moreover, since we want to define a learning rate scheduler, we define a custom \"initalize_model\" function as follows.\n",
    "\n",
    "# Function for initializing the BERT Classifier model, optimizer, and learning rate scheduler\n",
    "def initialize_roberta_model(epochs):\n",
    "    # Instantiate Bert Classifier\n",
    "    roberta_classifier = Roberta_Classifier(freeze_bert=False)\n",
    "\n",
    "    roberta_classifier.to(device)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(roberta_classifier.parameters(),\n",
    "                      lr=2e-05,    # learning rate, set to default value\n",
    "                      eps=1e-8    # decay, set to default value\n",
    "                      )\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Define the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return roberta_classifier, optimizer, scheduler\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS=2\n",
    "\n",
    "#And then we intialize the BERT model calling the \"initialize_model\" function we defined.\n",
    "\n",
    "roberta_classifier, roberta_optimizer, roberta_scheduler = initialize_roberta_model(epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237b77e-f954-4479-a73f-35a8c46f3ac5",
   "metadata": {},
   "source": [
    "#### ROBERTA Training\n",
    "After defining the custom BERT classifier model, we are ready to start the training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9fc32dd-9d57-44cf-adec-2662c620ffbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Define Cross entropy Loss function for the multiclass classification task\\nloss_fn = nn.CrossEntropyLoss()\\n\\ndef train(model, train_dataloader, optimizer, scheduler, epochs, val_dataloader=None, evaluation=False):\\n\\n    print(\"Start training...\\n\")\\n    for epoch_i in range(epochs):\\n        print(\"-\"*10)\\n        print(\"Epoch : {}\".format(epoch_i+1))\\n        print(\"-\"*10)\\n        print(\"-\"*38)\\n        print(f\"{\\'BATCH NO.\\':^7} | {\\'TRAIN LOSS\\':^12} | {\\'ELAPSED (s)\\':^9}\")\\n        print(\"-\"*38)\\n\\n        # Measure the elapsed time of each epoch\\n        t0_epoch, t0_batch = time.time(), time.time()\\n\\n        # Reset tracking variables at the beginning of each epoch\\n        total_loss, batch_loss, batch_counts = 0, 0, 0\\n        \\n        ###TRAINING###\\n\\n        # Put the model into the training mode\\n        model.train()\\n\\n        for step, batch in enumerate(train_dataloader):\\n            batch_counts +=1\\n            \\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\\n\\n            # Zero out any previously calculated gradients\\n            model.zero_grad()\\n\\n            # Perform a forward pass and get logits.\\n            logits = model(b_input_ids, b_attn_mask)\\n\\n            # Compute loss and accumulate the loss values\\n            loss = loss_fn(logits, b_labels)\\n            batch_loss += loss.item()\\n            total_loss += loss.item()\\n\\n            # Perform a backward pass to calculate gradients\\n            loss.backward()\\n\\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\\n            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n\\n            # Update model parameters:\\n            # fine tune BERT params and train additional dense layers\\n            optimizer.step()\\n            # update learning rate\\n            scheduler.step()\\n\\n            # Print the loss values and time elapsed for every 100 batches\\n            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\\n                # Calculate time elapsed for 20 batches\\n                time_elapsed = time.time() - t0_batch\\n                \\n                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\\n\\n                # Reset batch tracking variables\\n                batch_loss, batch_counts = 0, 0\\n                t0_batch = time.time()\\n\\n        # Calculate the average loss over the entire training data\\n        avg_train_loss = total_loss / len(train_dataloader)\\n\\n        ###EVALUATION###\\n        \\n        # Put the model into the evaluation mode\\n        model.eval()\\n        \\n        # Define empty lists to host accuracy and validation for each batch\\n        val_accuracy = []\\n        val_loss = []\\n\\n        for batch in val_dataloader:\\n            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\\n            \\n            # We do not want to update the params during the evaluation,\\n            # So we specify that we dont want to compute the gradients of the tensors\\n            # by calling the torch.no_grad() method\\n            with torch.no_grad():\\n                logits = model(batch_input_ids, batch_attention_mask)\\n\\n            loss = loss_fn(logits, batch_labels)\\n\\n            val_loss.append(loss.item())\\n\\n            # Get the predictions starting from the logits (get index of highest logit)\\n            preds = torch.argmax(logits, dim=1).flatten()\\n\\n            # Calculate the validation accuracy \\n            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\\n            val_accuracy.append(accuracy)\\n\\n        # Compute the average accuracy and loss over the validation set\\n        val_loss = np.mean(val_loss)\\n        val_accuracy = np.mean(val_accuracy)\\n        \\n        # Print performance over the entire training data\\n        time_elapsed = time.time() - t0_epoch\\n        print(\"-\"*61)\\n        print(f\"{\\'AVG TRAIN LOSS\\':^12} | {\\'VAL LOSS\\':^10} | {\\'VAL ACCURACY (%)\\':^9} | {\\'ELAPSED (s)\\':^9}\")\\n        print(\"-\"*61)\\n        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\\n        print(\"-\"*61)\\n        print(\"\\n\")\\n    \\n    print(\"Training complete!\")\\n\\n#Roberta Training\\ntrain(roberta_classifier, train_dataloader,roberta_optimizer, roberta_scheduler, EPOCHS, val_dataloader)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Define Cross entropy Loss function for the multiclass classification task\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, optimizer, scheduler, epochs, val_dataloader=None, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\"-\"*10)\n",
    "        print(\"Epoch : {}\".format(epoch_i+1))\n",
    "        print(\"-\"*10)\n",
    "        print(\"-\"*38)\n",
    "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*38)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "        ###TRAINING###\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            \n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass and get logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters:\n",
    "            # fine tune BERT params and train additional dense layers\n",
    "            optimizer.step()\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 100 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        ###EVALUATION###\n",
    "        \n",
    "        # Put the model into the evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Define empty lists to host accuracy and validation for each batch\n",
    "        val_accuracy = []\n",
    "        val_loss = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "            \n",
    "            # We do not want to update the params during the evaluation,\n",
    "            # So we specify that we dont want to compute the gradients of the tensors\n",
    "            # by calling the torch.no_grad() method\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            # Get the predictions starting from the logits (get index of highest logit)\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "            # Calculate the validation accuracy \n",
    "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "            val_accuracy.append(accuracy)\n",
    "\n",
    "        # Compute the average accuracy and loss over the validation set\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_accuracy = np.mean(val_accuracy)\n",
    "        \n",
    "        # Print performance over the entire training data\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*61)\n",
    "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
    "        print(\"-\"*61)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "#Roberta Training\n",
    "train(roberta_classifier, train_dataloader,roberta_optimizer, roberta_scheduler, EPOCHS, val_dataloader)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "690555c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "----------\n",
      "Epoch : 1\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 101/784 [11:39<1:17:45,  6.83s/it, train_loss=0.956]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100    |   0.956399   |  699.37  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 201/784 [22:44<1:04:12,  6.61s/it, train_loss=0.672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   200    |   0.385137   |  664.76  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 301/784 [34:00<55:13,  6.86s/it, train_loss=0.55]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   300    |   0.304871   |  676.70  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 401/784 [45:22<42:31,  6.66s/it, train_loss=0.489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   400    |   0.304346   |  682.09  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▍   | 501/784 [56:21<30:49,  6.54s/it, train_loss=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500    |   0.298851   |  659.08  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 601/784 [1:07:20<20:19,  6.66s/it, train_loss=0.42] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   600    |   0.267284   |  658.63  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  89%|████████▉ | 701/784 [1:18:20<09:41,  7.00s/it, train_loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   700    |   0.285680   |  659.86  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 784/784 [1:27:42<00:00,  6.71s/it, train_loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   783    |   0.261819   |  561.61  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.386408    |  0.232688  |       92.82       |  5553.12 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------\n",
      "Epoch : 2\n",
      "----------\n",
      "--------------------------------------\n",
      "BATCH NO. |  TRAIN LOSS  | ELAPSED (s)\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  13%|█▎        | 101/784 [11:32<1:17:06,  6.77s/it, train_loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100    |   0.229102   |  692.23  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  26%|██▌       | 201/784 [22:43<1:04:17,  6.62s/it, train_loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   200    |   0.187900   |  670.96  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 301/784 [33:44<53:23,  6.63s/it, train_loss=0.215]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   300    |   0.226992   |  660.88  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  51%|█████     | 401/784 [44:38<42:27,  6.65s/it, train_loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   400    |   0.223603   |  654.81  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  64%|██████▍   | 501/784 [55:35<30:50,  6.54s/it, train_loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   500    |   0.203608   |  656.81  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  77%|███████▋  | 601/784 [1:06:32<19:42,  6.46s/it, train_loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   600    |   0.197479   |  657.10  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  89%|████████▉ | 701/784 [1:17:29<09:06,  6.59s/it, train_loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   700    |   0.193903   |  656.44  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 784/784 [1:26:37<00:00,  6.63s/it, train_loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   783    |   0.186227   |  548.06  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "AVG TRAIN LOSS |  VAL LOSS  | VAL ACCURACY (%) | ELAPSED (s)\n",
      "-------------------------------------------------------------\n",
      "   0.206562    |  0.205949  |       93.95       |  5492.50 \n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "# Define Cross entropy Loss function for the multiclass classification task\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_dataloader, optimizer, scheduler, epochs, val_dataloader=None, evaluation=False):\n",
    "\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        print(\"-\"*10)\n",
    "        print(\"Epoch : {}\".format(epoch_i+1))\n",
    "        print(\"-\"*10)\n",
    "        print(\"-\"*38)\n",
    "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n",
    "        print(\"-\"*38)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        \n",
    "        ###TRAINING###\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch_i+1}\")\n",
    "\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            batch_counts += 1\n",
    "            \n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass and get logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update model parameters:\n",
    "            # fine tune BERT params and train additional dense layers\n",
    "            optimizer.step()\n",
    "            # update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 100 batches\n",
    "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 100 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "            # Update the progress bar description with the current loss\n",
    "            progress_bar.set_postfix({'train_loss': total_loss / (step + 1)})\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        ###EVALUATION###\n",
    "        \n",
    "        if evaluation and val_dataloader is not None:\n",
    "            # Put the model into the evaluation mode\n",
    "            model.eval()\n",
    "            \n",
    "            # Define empty lists to host accuracy and validation for each batch\n",
    "            val_accuracy = []\n",
    "            val_loss = []\n",
    "\n",
    "            for batch in val_dataloader:\n",
    "                batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n",
    "                \n",
    "                # We do not want to update the params during the evaluation,\n",
    "                # So we specify that we dont want to compute the gradients of the tensors\n",
    "                # by calling the torch.no_grad() method\n",
    "                with torch.no_grad():\n",
    "                    logits = model(batch_input_ids, batch_attention_mask)\n",
    "\n",
    "                loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                # Get the predictions starting from the logits (get index of highest logit)\n",
    "                preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "                # Calculate the validation accuracy \n",
    "                accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n",
    "                val_accuracy.append(accuracy)\n",
    "\n",
    "            # Compute the average accuracy and loss over the validation set\n",
    "            val_loss = np.mean(val_loss)\n",
    "            val_accuracy = np.mean(val_accuracy)\n",
    "        \n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(\"-\"*61)\n",
    "            print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n",
    "            print(\"-\"*61)\n",
    "            print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*61)\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Roberta Training\n",
    "train(roberta_classifier, train_dataloader, roberta_optimizer, roberta_scheduler, EPOCHS, val_dataloader, evaluation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ed2d571-fce5-4a9f-ab50-9201e921f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(roberta_classifier, \"../../data/RoBERTa/RoBERTa_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2fc14-086d-4120-a119-c54c5e27bb43",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98222d73-97cb-4a5f-90d9-497d33051686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def predict(model, test_dataloader):\n",
    "    \n",
    "    # Define empty list to host the predictions\n",
    "    preds_list = []\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Predicting\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_input_ids, batch_attention_mask)\n",
    "        \n",
    "        # Get index of highest logit\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        # Append predicted class to list\n",
    "        preds_list.extend(preds)\n",
    "    \n",
    "    return preds_list\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\"\"\"\n",
    "def predict(model, test_dataloader, portion=None):\n",
    "    \"\"\"\n",
    "    Predict the classes for the data in the test_dataloader.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - test_dataloader: DataLoader containing the test data.\n",
    "    - portion: The number of batches to consider from the test_dataloader. If None, consider all batches.\n",
    "\n",
    "    Returns:\n",
    "    - preds_list: List of predicted classes.\n",
    "    \"\"\"\n",
    "    # Define empty list to host the predictions\n",
    "    preds_list = []\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Predicting\")\n",
    "    \n",
    "    # Initialize a counter to limit the number of batches processed\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "        \n",
    "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_input_ids, batch_attention_mask)\n",
    "        \n",
    "        # Get index of highest logit\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        # Append predicted class to list\n",
    "        preds_list.extend(preds)\n",
    "        \n",
    "        # Increment the batch counter\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Check if we have processed the specified portion of the dataloader\n",
    "        if portion is not None and batch_count >= portion:\n",
    "            break\n",
    "    \n",
    "    return preds_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "800a0160-6d5c-4fc0-97ab-6d26d2fa6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 245/245 [06:11<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "roberta_preds = predict(roberta_classifier, test_dataloader)\n",
    "#roberta_preds = predict(roberta_classifier, test_dataloader, portion=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76fd75bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7837\n",
      "7837\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(len(roberta_preds))\n",
    "print(len(y_test))\n",
    "\n",
    "#print(len(y_test[:(len(roberta_preds))]))\n",
    "#y_test = y_test[:(len(roberta_preds))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3611102-197b-47c7-b31f-f62de33b2c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for ROBERTA :\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "not_cyberbullying       0.84      0.89      0.86      1542\n",
      "           gender       0.93      0.87      0.90      1533\n",
      "         religion       0.95      0.97      0.96      1593\n",
      "              age       0.99      0.98      0.98      1590\n",
      "        ethnicity       0.99      0.97      0.98      1579\n",
      "\n",
      "         accuracy                           0.94      7837\n",
      "        macro avg       0.94      0.94      0.94      7837\n",
      "     weighted avg       0.94      0.94      0.94      7837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report for ROBERTA :\\n', classification_report(y_test, roberta_preds, target_names=possible_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbf91ca4-0154-4aee-9e57-4716961bf35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['not_cyberbullying', 'gender', 'religion', 'age', 'ethnicity']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the classification report\n",
    "report = classification_report(y_test, roberta_preds, target_names=possible_labels, output_dict=True)\n",
    "\n",
    "# Extract categories for classification (excluding the overall metrics like 'accuracy', 'macro avg', 'weighted avg')\n",
    "categories = [label for label in possible_labels if label in report]\n",
    "print(\"Categories:\", categories)\n",
    "\n",
    "# Extracting precision, recall, and f1-score\n",
    "precision = [report[category]['precision'] for category in categories]\n",
    "recall = [report[category]['recall'] for category in categories]\n",
    "f1_score = [report[category]['f1-score'] for category in categories]\n",
    "\n",
    "# Setting the positions and width for the bars\n",
    "pos = list(range(len(categories)))\n",
    "width = 0.25 \n",
    "\n",
    "# Plotting each metric\n",
    "plt.bar(pos, precision, width, alpha=0.5, color='#ff33cc', label='Precision')\n",
    "plt.bar([p + width for p in pos], recall, width, alpha=0.5, color='#6600ff', label='Recall')\n",
    "plt.bar([p + width*2 for p in pos], f1_score, width, alpha=0.5, color='#00cc99', label='F1-Score')\n",
    "\n",
    "# Adding the aesthetics\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Classification Report')\n",
    "\n",
    "# Ensure the number of x-ticks matches the number of categories\n",
    "plt.xticks([p + width for p in pos], categories)\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(['Precision', 'Recall', 'F1-Score'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec161f-f62b-4e4b-9529-ecd3f3faefd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
