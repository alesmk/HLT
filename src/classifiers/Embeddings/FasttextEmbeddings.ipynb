{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chiarapiccolo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fathers', 0.8293688893318176), ('grandfather', 0.8073168396949768), ('gather', 0.6720890402793884), ('mother', 0.604293704032898), ('grandmother', 0.5891567468643188), ('cure', 0.5657365918159485), ('fate', 0.5648766160011292), ('shah', 0.54682457447052), ('bfs', 0.5322533845901489), ('mothers', 0.5314666628837585)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Scarica risorse necessarie di NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Carica il dataset\n",
    "df = pd.read_csv('../../../data/New dataset/LSTM/no_preprocessing/train_tweets_LSTM_no_new.csv')\n",
    "\n",
    "# Funzione di preprocessing\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)  # Tokenizza il testo\n",
    "    return tokens\n",
    "\n",
    "# Applica il preprocessing\n",
    "df['tokenized_tweet'] = df['tweet_text'].apply(preprocess)\n",
    "\n",
    "# Crea una lista di liste di token\n",
    "tokenized_sentences = df['tokenized_tweet'].tolist()\n",
    "\n",
    "# Inizializza il modello FastText\n",
    "fast_model = FastText(vector_size=100, window=10, min_count=5, sg=1, negative=10)\n",
    "\n",
    "# Costruisci il vocabolario\n",
    "fast_model.build_vocab(corpus_iterable=tokenized_sentences)\n",
    "\n",
    "# Addestra il modello\n",
    "fast_model.train(corpus_iterable=tokenized_sentences, total_examples=len(tokenized_sentences), epochs=20)\n",
    "\n",
    "# Salva il modello addestrato (opzionale)\n",
    "fast_model.save('fasttext_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pussies', 0.6965669393539429), ('trash', 0.674778401851654), ('wayne', 0.6701973080635071), ('411pain', 0.6674219965934753), ('hoe', 0.6223486065864563), ('ass', 0.6170661449432373), ('vagina', 0.6070759296417236), ('fuck', 0.5998162627220154), ('yoself', 0.599560558795929), ('shutup', 0.599079430103302)]\n"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo del modello per trovare parole simili\n",
    "similar_words = fast_model.wv.most_similar('pussy')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', -0.08697278797626495)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_model.wv.most_similar(['mum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": capital-common-countries\n",
      "Athens Greece Baghdad Iraq\n",
      "Athens Greece Bangkok Thailand\n",
      "Athens Greece Beijing China\n",
      "Athens Greece Berlin Germany\n",
      "Athens Greece Bern Switzerland\n",
      "Athens Greece Cairo Egypt\n",
      "Athens Greece Canberra Australia\n",
      "Athens Greece Hanoi Vietnam\n",
      "Athens Greece Havana Cuba\n",
      "Athens Greece Helsinki Finland\n",
      "Athens Greece Islamabad Pakistan\n",
      "Athens Greece Kabul Afghanistan\n",
      "Athens Greece London England\n",
      "Athens Greece Madrid Spain\n",
      "Athens Greece Moscow Russia\n",
      "Athens Greece Oslo Norway\n",
      "Athens Greece Ottawa Canada\n",
      "Athens Greece Paris France\n",
      "Athens Greece Rome Italy\n",
      "Athens Greece Stockholm Sweden\n",
      "Athens Greece Tehran Iran\n",
      "Athens Greece Tokyo Japan\n",
      "Baghdad Iraq Bangkok Thailand\n",
      "Baghdad Iraq Beijing China\n",
      "Baghdad Iraq Berlin Germany\n",
      "Baghdad Iraq Bern Switzerland\n",
      "Baghdad Iraq Cairo Egypt\n",
      "Baghdad Iraq Canberra Australia\n",
      "Baghdad Iraq Hanoi Vietnam\n",
      "Baghdad Iraq Havana Cuba\n",
      "Baghdad Iraq Helsinki Finland\n",
      "Baghdad Iraq Islamabad Pakistan\n",
      "Baghdad Iraq Kabul Afghanistan\n",
      "Baghdad Iraq London England\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt\"\n",
    "test_file = 'questions-words.txt'\n",
    "questions = requests.get(url).content.decode()\n",
    "with open(test_file,mode='w',encoding='utf-8') as outputfile:\n",
    "    outputfile.write(questions)\n",
    "print(questions[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_wAp_analogy = fast_model.wv.evaluate_word_analogies(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per Section:\n",
      "                        Section  Accuracy\n",
      "0      capital-common-countries  0.000000\n",
      "1                 capital-world  0.000000\n",
      "2                      currency  0.000000\n",
      "3                 city-in-state  0.000000\n",
      "4                        family  0.128205\n",
      "5     gram1-adjective-to-adverb  0.754167\n",
      "6                gram2-opposite  1.000000\n",
      "7             gram3-comparative  0.400735\n",
      "8             gram4-superlative  0.311111\n",
      "9      gram5-present-participle  0.266082\n",
      "10  gram6-nationality-adjective  0.560000\n",
      "11             gram7-past-tense  0.104615\n",
      "12                 gram8-plural  0.688312\n",
      "13           gram9-plural-verbs  0.447619\n",
      "14               Total accuracy  0.391387\n"
     ]
    }
   ],
   "source": [
    "# Extracting the accuracy and category results\n",
    "total_accuracy = fasttext_wAp_analogy[0]\n",
    "category_results = fasttext_wAp_analogy[1]\n",
    "\n",
    "# Processing the data\n",
    "sections = []\n",
    "accuracies = []\n",
    "total_correct = 0\n",
    "total_incorrect = 0\n",
    "\n",
    "for entry in category_results:\n",
    "    section = entry['section']\n",
    "    correct = len(entry['correct'])\n",
    "    incorrect = len(entry['incorrect'])\n",
    "    total = correct + incorrect\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    sections.append(section)\n",
    "    accuracies.append(accuracy)\n",
    "    total_correct += correct\n",
    "    total_incorrect += incorrect\n",
    "\n",
    "# Total accuracy\n",
    "total_accuracy = total_correct / (total_correct + total_incorrect)\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Section': sections,\n",
    "    'Accuracy': accuracies\n",
    "})\n",
    "\n",
    "print(\"Accuracy per Section:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accuracy: 0.3914\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m category_results \u001b[38;5;241m=\u001b[39m fasttext_wAp_analogy[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category, results \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcategory_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m      8\u001b[0m     correct \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     incorrect \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincorrect\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stampa i risultati\n",
    "total_accuracy = fasttext_wAp_analogy[0]\n",
    "category_results = fasttext_wAp_analogy[1]\n",
    "\n",
    "print(f\"Total Accuracy: {total_accuracy:.4f}\")\n",
    "\n",
    "for category, results in category_results.items():\n",
    "    correct = results['correct']\n",
    "    incorrect = results['incorrect']\n",
    "    accuracy = correct / (correct + incorrect) if (correct + incorrect) > 0 else 0\n",
    "    print(f\"Category: {category}, Correct: {correct}, Incorrect: {incorrect}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/New dataset/LSTM/no_preprocessing/train_tweets_LSTM_no_new.csv')\n",
    "df['tweet_text'].to_csv('text_data.txt', index=False, header=False)\n",
    "\n",
    "test_file = 'questions-words.txt'\n",
    "\n",
    "\n",
    "# Addestra il modello FastText\n",
    "model = fasttext.train_unsupervised('text_data.txt', model='skipgram', dim=300, minCount=5, epoch=10)\n",
    "\n",
    "def evaluate_word_analogies(model, analogy_file):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with open(analogy_file, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            if len(words) != 4:\n",
    "                continue  # Skip lines that don't have exactly 4 words\n",
    "            word1, word2, word3, word4 = words\n",
    "            \n",
    "            # Ensure all words are in the vocabulary\n",
    "            if all(word in model.words for word in [word1, word2, word3, word4]):\n",
    "                predicted_word = model.get_analogies(word1, word2, word3, 1)[0][1]\n",
    "                if predicted_word == word4:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Esempio di analogia: king - man + woman = ?\n",
    "accuracy = evaluate_word_analogies(model, 'questions-words.txt')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cc9b73884fc6dc94cb042d029db4d21e2f9331e7b6a549688c5b94abae7d991"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('ispr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
