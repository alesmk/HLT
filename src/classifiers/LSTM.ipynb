{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import  RandomizedSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import fasttext ###\n",
    "from keras.regularizers import L2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../data/New dataset/LSTM/no_preprocessing/train_tweets_LSTM_no_new.csv')\n",
    "df_val = pd.read_csv('../../data/New dataset/LSTM/no_preprocessing/eval_tweets_LSTM_no_new.csv')\n",
    "df_test = pd.read_csv('../../data/New dataset/LSTM/no_preprocessing/test_tweets_LSTM_no_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train[\"tweet_text\"], df_train[\"cyberbullying_type\"]\n",
    "X_val, y_val = df_val[\"tweet_text\"], df_val[\"cyberbullying_type\"]\n",
    "X_test, y_test = df_test[\"tweet_text\"], df_test[\"cyberbullying_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tweet_length'] = df_train['tweet_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_train['tweet_length'], bins=30, kde=True)\n",
    "plt.title('Tweet Length distribution')\n",
    "plt.xlabel('Tweet length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text_len'] = [len(text.split()) for text in df_train.tweet_text]\n",
    "max_len = 45\n",
    "count = (df_train['text_len'] >= max_len).sum() \n",
    "total_tweets = len(df_train)\n",
    "percentage = (count / total_tweets) * 100\n",
    "print(f\"Percentage of tweets longer than {max_len} tokens: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform label encoder on the target variable\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "#Each word in input used as a key, while a unique index is used as the value of the key \n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)\n",
    "X_val = word_tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"The vocaboulary length is:\", vocab_length)\n",
    "\n",
    "X_train = pad_sequences(X_train, padding = 'pre', maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, padding = 'pre', maxlen = max_len)\n",
    "X_val = pad_sequences(X_val, padding = 'pre', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word embeddings and create a dictionary that willl contain words as keys, and their corresponging embedding list as values. \n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('Embeddings/glove.twitter.27B.200d.txt', encoding=\"utf8\") ## ATTENZIONE: change if you change emebdding dim\n",
    "\n",
    "for line in glove_file:\n",
    "\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 200)) ## ATTENZIONE: change if the dimention of embedding changes above\n",
    "\n",
    "i = 0\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "print(\"The number of out-of-vocabulary words is:\", i)\n",
    "print(\"The percentage of out-of-vocabulary words is:\", (i/vocab_length) * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embedding matrix \n",
    "#embedding_matrix = np.load(\"Embeddings/fasttext_embeddingmatrix_no.npy\") \n",
    "\n",
    "#num_words, embedding_dim = embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking everything is working properly\n",
    "\n",
    "print(X_train[1])\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}')\n",
    "print(f'y_val shape: {y_val.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - no dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and RandomizedGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=False, #Depending on which embeddings we use\n",
    "                        mask_zero = True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(64, activation='relu')) #regolarizzazione: diminuisci i features prima di generare output\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='baseline_nodense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE layer o NO \n",
    "    project_name='no_200_rs' ### Inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "As suggested on the Keras official documentation, we retrain the model using the best parameters found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model = build_model(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation and RandomizedGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_dense(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=False, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero=True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01)))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_dense,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='baseline_dense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='no_200_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_dense = build_model_dense(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_dense.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_dense.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_dense.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (Bidirectional Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional - no dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bi(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=False, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero=True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(Bidirectional(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01))))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "    # RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_bi,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='bidirectional_nodense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='no_200_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_bi = build_model_bi(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_bi.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_bi.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_bi.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional - dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bi_dense(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_length, \n",
    "                        output_dim=embedding_matrix.shape[1], \n",
    "                        weights=[embedding_matrix],  \n",
    "                        trainable=False, # ATTENZIONE: Depending on which embeddings we use\n",
    "                        mask_zero=True))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.3, max_value=0.5, step=0.1)))\n",
    "    model.add(Bidirectional(LSTM(units=hp.Choice('units', values=[16, 32, 64]), kernel_regularizer=L2(0.01))))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "    # RandomSearch configuration\n",
    "tuner = RandomSearch(\n",
    "    build_model_bi_dense,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,  # Max numbers of different configurations to test\n",
    "    executions_per_trial=1,  # Number of times each config is executed (different weigths initialization)\n",
    "    overwrite=True,\n",
    "    directory='bidirectional_dense_rs', ## ATTENZIONE: cambiare a seconda della configuraizone che proviamo (inserire tipo LSTM: baseline, bidirectional, attention layer) + DENSE o NO \n",
    "    project_name='no_200_rs' ### ATTENZIONE: inserire config parametri: dataset (pre o no), num_embeddings\n",
    ")\n",
    "\n",
    "# Callback di EarlyStopping\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "fixed_batch_size = 64\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=100, \n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=[callback],\n",
    "             batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the search \n",
    "\n",
    "num_trials = len(tuner.oracle.trials)\n",
    "print(f'Number of evaluated configurations: {num_trials}')\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 2 hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hp.\n",
    "model_bi_dense = build_model_bi_dense(best_hps[0])\n",
    "\n",
    "# Model training\n",
    "history = model_bi_dense.fit(x = X_train, y = y_train, \n",
    "                         epochs=50,\n",
    "                         validation_data = [X_val, y_val],\n",
    "                         callbacks = [callback],\n",
    "                         batch_size=fixed_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and accuracy plots for training and validation\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['acc'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_bi_dense.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_bi_dense.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Class Names: \", class_names)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cc9b73884fc6dc94cb042d029db4d21e2f9331e7b6a549688c5b94abae7d991"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('ispr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
